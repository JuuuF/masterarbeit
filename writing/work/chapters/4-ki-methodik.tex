% !TEX root = ../main.tex

\section{Methodik}
\label{sec:ki:methodik}

\todo{Einleitende Sätze zur Methodik.}

% -------------------------------------------------------------------------------------------------

\subsection{Die YOLOv8*-Architektur}
\label{sec:warum_yolov8}

Die Vorhersagen der Dartpfeilpositionen in normalisierten Bildern geschieht durch ein neuronales Netz der YOLOv8*-Architektur. Diese Architektur basiert auf YOLOv8 \cite{yolov8_paper}, welche für den Einsatz der Dartpfeilerkennung umstrukturiert wurde.

Dieses Unterkapitel thematisiert zunächst die Hintergründe der Wahl von YOLOv8 als Basismodell in \autoref{sec:hintergrund_yolov8}. Danach werden in \autoref{sec:yolo_adaption} vorgenommene Adaptionen an der Architektur aufgezeigt. Im Anschluss wird der konkrete Aufbau von YOLOv8* in \autoref{sec:yolov8_aufbau} dargestellt.

\subsubsection{Hintergründe zur Wahl von YOLOv8 als Basismodell}
\label{sec:hintergrund_yolov8}

Mit YOLOv8 wurde eine Netzwerkarchitektur vorgestellt, die in der Lage ist, Objekterkennung durch Single-Shot-Vorhersagen effizient und mit hoher Genauigkeit auszuführen \cite{yolov8_paper}. Die vorgestellte Netzwerkarchitektur ist in der Lage, in Echtzeit ausgeführt zu werden und selbst in ressourcenbegrenzten Umgebungen hohe Genauigkeiten zu erlangen.

Die Architektur von YOLOv8 ist derart parametrisiert, dass unterschiedliche Netzwerkgrößen als Varianten vorgegeben sind. Die Größe und Komplexität des Netzwerks ist aufgeteilt in die Klassen n (nano), s (small), m (medium), l (large) und x (extra large). Diese Varianten unterscheiden sich in der Anzahl den Größen und Anzahlen der verwendeten Schichten und sind für unterschiedliche Einsatzsituationen ausgelegt. Während l- und x-Modelle für Umgebungen ausgelegt sind, in denen vorhandene Rechenleistung keinen Engpass darstellt, sind die s- und n-Modelle für den Einsatz in mobilen Geräten oder Edge-Devices vorgesehen, in denen die Ressourcen begrenzt sind. Die Verwendung von Netzwerken geringerer Größen geht mit Einbußen in der Qualität der Vorhersagen einher.

Zusätzlich zu den genannten Charakteristiken ist YOLOv8 ein optimierter Nachfolger der für DeepDarts verwendeten YOLOv4-Architektur. Durch den Einsatz von YOLOv4 in DeepDarts konnte die Fähigkeit dieser Familie der Netzwerkarchitekturen hinsichtlich der Erkennung von Dartpfeilen gezeigt werden.

Aufgrund des vorgesehenen Einsatzbereichs des Systems dieser Arbeit in mobilen Endgeräten ist die flexible Netzwerkgröße in Kombination mit einer hohen Qualität der Vorhersagen ausschlaggebend für die Entscheidung, diese Netzwerkarchitektur als Basismodell für die Ausarbeitung dieser Arbeit zu verwenden.

\subsubsection{Adaption des Modells}
\label{sec:yolo_adaption}

Obgleich durch \citeauthor{deepdarts} der Einsatz von YOLO-Architekturen zur Identifizierung von Dartpfeilen gezeigt wurde ist die generelle Strukturierung der Architektur nicht optimal für die zugrundeliegende Aufgabe. Für eine Abstimmung von Architektur und Aufgabe wurden strukturelle Änderungen im Netzwerkaufbau unternommen. Die Adaptierte Netzwerkarchitektur wird im Folgenden als YOLOv8* bezeichnet, um eine Differenzierung zwischen zu der offiziellen YOLOv8-Architektur herzustellen.

\paragraph{Bounding Boxes}

Im Wesentlichen ist die Verwendung von Ankerpunkten mit umliegenden Bounding Boxes bei der Lokalisierung von Objekten in Bildern effektiv und zielführend. Bei der Vorhersage spezifischer Positionen in einem Bild liefert die Verwendung von Bounding Boxes jedoch keinen signifikanten Vorteil. Das DeepDarts-System projizierte quadratische Bounding Boxes auf die Dartpfeilspitzen, um Outputdaten zum Training des Netzwerks zu generieren. Diese Bounding Boxes wurden im Verlauf des Trainings verkleinert, bis sie eine minimale Größe erreichten \cite{deepdarts}. Die letztendliche Positionsberechnung der identifizierten Dartpfeile bezog die Bounding Boxes nicht mit ein, da der Ankerpunkt der quadratischen Bounding Box die Position des Dartpfeils angibt. Die YOLOv8*-Architektur wurde an ihren Einsatz angepasst, indem die Verwendung von Bounding Boxes aus der Architektur eliminiert wurde.

\paragraph{Multi-Scale-Output}

In der YOLOv8-Architektur wird ein Multi-Scale-Output zur Identifizierung von Objekten unterschiedlicher Größen eingesetzt. Objekte werden in 3 unterschiedlichen Kontextgrößen identifiziert; durch Nachverarbeitungsschritte werden diese Outputs miteinander kombiniert. Dieser Multi-Scale-Output ist in der YOLOv8*-Architektur nicht vorhanden, da für die zu identifizierenden Objekte keine starken Größenvariationen zu erwarten sind. Zudem unterliegt eine kleine Kontextgröße der Gefahr der fehlerhaften Klassifizierung von Abnutzungen der Dartscheibe als Dartpfeil. Der Kontext des gesamten Dartpfeils ist zur Identifizierung seines Einstichpunktes notwendig; dieser ist durch die Normalisierung der Dartscheibe auf eine feste Größe von $800 \times 800\,\text{px}$ durch den größten Kontext gegeben.

\paragraph{Dreiteilungen der Outputs}

Die YOLOv8*-Architektur unterteilt das Eingabebild in Regionen und bestimmt die Existenz von Dartpfeilen je Region. Da eine typische Runde Darts aus 3 Würfen besteht, ist davon auszugehen, dass eine maximale Anzahl von 3 Dartpfeilen in den Bildern zu identifizieren sind. Dabei kann jedoch nicht ausgeschlossen werden, dass sich die Dartpfeilspitzen in unterschiedlichen Regionen befinden. Je Region wird daher eine feste Anzahl von 3 möglichen Dartpfeilpositionen vorhergesagt. Existiert lediglich ein Dartpfeil, so sind zwei mögliche Outputs genullt.

\paragraph{Transition-Block}

Eine grundlegende Änderung der YOLOv8-Architektur ist das Hinzufügen von Transition-Blocks. Diese befinden sich an den Übergängen zwischen Backbone und Head sowie Head und Detect. Sie brechen den Fully-Convolutional-Approach der YOLO-Netzwerke durch Einbindung von Dense-Schichten. Die Eigenschaft von Fully-Connected-CNNs, Bilder beliebiger Eingabegrößen verarbeiten zu können, geht mit der Einschränkung einher, keinen direkten globalen Kontext des Bildes einzufangen. Da die Eingabegrößen der Bilder der in dieser Thesis erarbeiteten Systems durch die Vorverarbeitung vorgegeben sind, liefert die Verarbeitung beliebiger Eingabegrößen keinen Mehrwert und ermöglicht damit die Lockerung dieses Paradigmas. Folglich ist eine Einbindung von Fully-Connected-Schichten architektonisch möglich und ermöglicht einen globalen Überblick über das Eingabebild, anhand derer Informationen der gesamten Dartscheibe in untergeordnete Abschnitte des Netzwerks einfließen können. Die Einschränkung auf lokale Kontextfenster wird dadurch in der YOLOv8*-Architektur aufgehoben.

\subsubsection{Aufbau der Architektur}
\label{sec:yolov8_aufbau}

Der Aufbau des in dieser Thesis verwendeten YOLOv8*-Architektur ist in \autoref{img:yolov8_architektur} dargestellt. Sie ist unterteilt in die Bereiche Backbone, Head und Detect. Das Backbone und der Head sind weitestgehend analog zur YOLOv8-Architektur strukturiert. Der Detection wurde ein gesonderter Netzwerkabschnitt zugeteilt, in welchem die Eliminierung des Multi-Scale-Outputs sowie die gesonderte Handhabung der Outputs manifestiert ist. Der Detection-Bereich ist dazu unterteilt in drei parallele Stränge, in welchen Auswertungen zu Existenz, Position und Klasse abgeleitet und zu einem gemeinsamen Output konkateniert werden. Dieser Output besitzt eine Größe von $25 \times 25 \times 8 \times 3$, wie in \autoref{img:datenformat} dargestellt.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/yolov8.pdf}
    \caption{YOLOv8*-Architektur. (1) Bottleneck; Extraktion von Features. (2) Head; Kombination von Features. (3) Detect; Deutung von Features.}
    \label{img:yolov8_architektur}
\end{figure}

Die Netzwerkarchitektur setzt sich aus unterschiedlichen Blöcken zusammen. Grundlegende Blöcke sind: Faltung (Conv2d), Max-Pooling (MaxPool2d), Addition (Add), zweidimensionale Normalisierung (BatchNorm2d), SiLU-Aktivierungsfunktion (SiLU) sowie Zweiteilung von Feature-Maps (Split) und Konkatenation von Feature Maps (Concat). Zusätzlich zu diesen bereits in YOLOv8 vorhandenen Grundblöcken werden Fully-Connected-Schichten (Dense), Dropout-Schichten (Dropout) und explizite Formänderungen der Tensoren (Reshape) ergänzt.

Aus diesen Grundblöcken werden weitere, größere Netzwerkblöcke zusammengesetzt, aus der die YOLOv8*-Architektur aufgebaut ist. Bereits in YOLOv8 enthaltene Blöcke sind SPPF, C2f und Bottleneck mit und ohne Shortcut. Adaptiert wurde der Conv-Block, indem eine Dropout-Schicht hinzugefügt wurde. Als neuer Block ist der Transition-Block aufgenommen worden, der ähnlich zu dem Bottleneck-Block mit Shortcut aufgebaut ist, jedoch mit der Addition eines Dense-Layers. Die zusammengesetzten Blöcke der YOLOv8*-Architektur sind in \autoref{img:yolov8_parts} dargestellt.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/yolov8_parts.pdf}
    \caption{Netzwerkbestandteile der YOLOv8*-Architektur. (1) SPPF-Block; dieser zeichnet sich durch Hintereinanderreihung von MaxPool2d-Schichten aus. (2) C2f-Block; dieser spaltet die Feature Maps auf und wendet wiederholt Bottleneck-Blöcke an. (3, 4) Bottleneck-Block ohne und mit Shortcut; wiederholte Anwendung von Convolution, ggf. mit residualem Shortcut. (5) Conv-Block; dieser stellt den Grundbaustein der Convolution dar. (6) Transition-Block; Residualer Fully-Connected layer zum Einfangen globaler Informationen.}
    \label{img:yolov8_parts}
\end{figure}

\subsubsection{Verwendete Konfiguration von YOLOv8*}
\label{sec:yolov8_konfiguration}

\begin{table}[t]
    \begin{tabular}{c||c|c|c}
        Konfiguration & Anzahl der Schichten & Trainierbare Parameter & Gesamtparameter \\ \hline
        n             & 188                  & 3,21 M                 & 3,23 M          \\
        s             & 188                  & 10,48 M                & 10,51 M         \\
        m             & 234                  & 18,24 M                & 18,27 M         \\
        l             & 280                  & 25,06 M                & 25,10 M         \\
        x             & 280                  & 38,70 M                & 38,75 M
    \end{tabular}
    \caption{Parameter- und Schichtzahlen unterschiedlicher Konfigurationen der YOLOv8*-Architektur.}
    \label{tab:yolov8_params}
\end{table}

Die variable Größe von YOLOv8 ist analog in YOLOv8* übernommen worden, sodass die Konfigurationen n, s, m, l und x möglich sind. Die verwendete Architektur dieser Thesis orientiert sich in ihrer Größenordnung an der in DeepDarts verwendeten Architektur YOLOv4-tiny, welche ca. 6 Millionen Parameter beinhaltete. Die Größenordnungen der Parametrisierungen sind in \autoref{tab:yolov8_params} dargestellt. Aufgrund der in Vergleich zu DeepDarts komplexeren Daten wurde sich für die Verwendung von YOLOv8*-s entschieden, welche mit etwa $10,5 \text{Millionen}$ Parametern etwa $40\%$ mehr Parameter als YOLOv4-tiny besitzt.

% -------------------------------------------------------------------------------------------------

\subsection{Loss-Funktionen}
\label{sec:losses}

Die Loss-Funktion, die für das Training der YOLOv8*-Architektur verwendet wurde, wurde nach dem Vorbild bereits existierender Loss-Funktionen für Netzwerke der YOLO-Familie konstruiert. Die Loss-Funktion setzt sich aus unterschiedlichen Teil-Losses zusammen, die zu einem gemeinsamen Loss kombiniert werden. Der Aufbau des gemeinsamen Losses durch die untergeordneten Losses folgt dabei der Architektur des Netzwerks.

\subsubsection{Zusammensetzung des Losses}

Die Loss-Funktion zum Trainieren der YOLOv8*-Architektur folgt dem Aufbau dieser. Sie ist definiert als gewichtete Summe aus Existenz-Loss, Klassen-Loss und Positions-Loss:
\[ \mathcal{L}(y, \hat{y}) = \omega_\text{xst}\,\mathcal{L}_\text{xst}(y, \hat{y}) + \omega_\text{cls}\,\mathcal{L}_\text{cls}(y, \hat{y}) + \omega_\text{pos}\,\mathcal{L}_\text{pos}(y, \hat{y}) \]
\paragraph{Existenz-Loss $\mathcal{L}_\text{xst}$}

Für den Existenz-Loss $\mathcal{L}_\text{xst}$ wird der Focal-Loss der Existenz-Einträge aller Einträge aller Regionen gebildet \cite{focal_loss}. Die Verwendung des Focal-Loss für diesen Loss liegt in der spärlichen Menge positiver Einträge in Output-Tensoren. Bei einer Auflösung von $25 \times 25$ Regionen und 3 Vorhersagen je Region besitzt der Output-Tensor $n_\text{entries} = 25 \times 25 \times 3 = 1875$ Einträge. Zu erwarten sind $ n_\text{positive} \leq 3 $ positive Einträge, sodass der maximal zu erwartende Prozentsatz positiver Einträge $p_\text{positive} = \frac{n_\text{positive}}{n_\text{entries}} \leq \frac{1}{625} = 0.16\% $. Der Focal-Loss gewichtet positive und negative Klassen, sodass ein gezieltes Erlernen trotz signifikanten Klassenungleichgewichts.

\paragraph{Klassen-Loss $\mathcal{L}_\text{cls}$}

Der Klassen-Loss $\mathcal{L}_\text{cls}$ beruht ebenso wie der Existenz-Loss auf dem Focal-Loss. Zur Berechnung des Losses werden die Regionen in Betracht gezogen, die einen Dartpfeil enthalten. Regionen ohne Dartpfeil werden nicht betrachtet, da diesen keine eindeutige Klasse zugeordnet werden kann. Es wird die kategorische Focal-Kreuzentropie (Categorical Focal Cross-Entropy) \cite{focal_loss} verwendet, um den Loss der für die Dartpfeile zugeteilten Klassen und den vorhergesagten Klassen in den jeweiligen Ziel-Regionen zu bestimmen. Die Klassen stehen für die Farben schwarz, weiß, rot und grün sowie einer Klasse zur Identifizierung des Außenbereichs, in dem keine Punkte erzielt werden (vgl. \autoref{img:datenformat}).

\paragraph{Positions-Loss $\mathcal{L}_\text{pos}$}

Die Bestimmung des Positions-Losses $\mathcal{L}_\text{pos}$ geschieht ebenso wie die Bestimmung von $\mathcal{L}_\text{cls}$ unter Berücksichtigung der annotierten Existenzen. Je Region werden Positionen normalisiert relativ zur oberen linken Ecke angegeben. Die Differenz der normalisierten, lokalen Positionen der Vorhersagen und Wahrheitswerte werden in je $x$- und $y$-Komponente berechnet und aufsummiert. Diese kombinierte Summe wird durch die Anzahl der vorhandenen Dartpfeile geteilt, um einen Mittelwert der Positionsabweichung zu berechnen. Existieren keine Positionen, beträgt der Loss-Wert 0.

\paragraph{Gewichtung}

Durch die Verwendung unterschiedlicher Losses für Existenz, Klasse und Position resultieren unterschiedliche Bildbereiche der Loss-Werte. Die Loss-Gewichte $\omega_\text{xst}$, $\omega_\text{cls}$ und $\omega_\text{pos}$ gewichten die Loss-Outputs derart, dass kein Loss wesentlich überwiegt und eine Reduktion des Losses $\mathcal{L}$ eine uniforme Reduktion der Losses $\mathcal{L}_\text{xst}$, $\mathcal{L}_\text{cls}$ und $\mathcal{L}_\text{pos}$ mit sich zieht. Zum Anpassen der Losses aneinander für eine gleichwertige Konvergenz wurden die gewichte $\omega_\text{xst} = 400$, $\omega_\text{cls} = 2000$ und $\omega_\text{pos} = 0.5$ verwendet.

\subsubsection{Hintergründe und Zielsetzung}

Der Hintergrund der Aufteilung des Losses in Existenz, Klasse und Position liegt in der Netzwerkarchitektur. Diese ist auf die Vorhersage von Existenz, Klasse und Position ausgelegt, um Dartpfeile für eine Punktzahlbestimmung zu lokalisieren. Durch die Kombination unterschiedlicher Losses mit eigenen Gewichten für je einen thematischen Bereich des Netzwerks ermöglicht ein ausgeglichenes und kontextbezogenes Training des gesamten Netzwerks. Auf diese Weise wird die Überschattung eines Teil-Losses durch einen anderen mit weitaus größerem Wert entgegengewirkt. Ein Overfitting eines Bereichs ist dadurch wahrscheinlicher in dem kombinierten Loss widergespiegelt, sodass auf diesen dynamisch während des Trainings eingegangen werden kann.

\subsubsection{Abweichung von DIoU-Loss}

Bei dem Training von YOLO-Architekturen ist die die Verwendung vom IoU-Losses oder Adaptionen wie CIoU, DIoU oder GIoU \cite{diou_losses}, eine übliche Praxis \cite{yolov1,yolov8_paper,yolo_training_giou}. Diese Erweiterungen des IoU-Losses stützen sich grundlegend auf der Annahme der Existenz von Bounding Boxes. Da diese in der YOLOv8*-Architektur eliminiert wurden, ist die Verwendung von IoU-basierten Loss-Funktionen obsolet. Es wurde jedoch mit einer Adaption des GIoU-Losses experimentiert, in der Quadrate vordefinierter Größe auf die vorhergesagten Positionen projiziert werden, anhand derer ein IoU-Loss möglich wäre. Durch diese Herangehensweise können viele Annahmen getroffen werden, durch die Optimierungen bezüglich Äquivalenz von GIoU-Loss und DIoU-Loss und effiziente Berechnung der Intersection-Area durch Distanzen der Punkte voneinander möglich sind. Da dieser Ansatz jedoch trotz Optimierungen mit einer großen Rechenleistung und starker Kongruenz zum Positions-Loss einherging, wurde diese Idee für das Training des Netzwerks verworfen.

% -------------------------------------------------------------------------------------------------

\subsection{Training}
\label{sec:nn_training}

Das Training des neuronalen Netzes zielt auf das Erlernen der Identifizierung von Dartpfeilspitzen in normalisierten Eingabebildern ab. Das Training stützt sich dabei auf die Verwendung synthetisch generierter Trainingsdaten, die durch sporadische Anreicherung durch echte Daten erweitert und durch starke Augmentierungstechniken vervielfältigt werden. Der Aufbau der Trainingsdaten, die Arten und die Verwendung von Augmentierung sowie der Trainingsablauf werden in den folgenden Unterkapiteln thematisiert und im Detail beschrieben.

\subsubsection{Trainingsdaten}
\label{sec:trainingsdaten}

Die Trainingsdaten bilden die Basis des Trainings eines neuronalen Netzes. Für das Training des DeepDarts-Systems wurden wenig diverse Daten verwendet, wodurch die Performance des Systems beeinträchtigt wurde, wie bereits in \autoref{sec:cv:ergebnisse} dargestellt wurde. Um diesem Phänomen der einseitigen und wenig diversen Daten entgegenzuwirken, wurde in dieser Thesis auf die Nutzung eigener, synthetisch generierter Daten gesetzt, die durch Salting echter Daten angereichert wurden. Die Datenerstellung erfolgte nach dem in \autoref{cha:daten} beschriebenen Prinzip. Zum Salting wurden einerseits Daten aus dem DeepDarts-Datensatz verwendet, sowie manuell aufgenommene Daten aus einem Lokal.

Die überwiegende Mehrheit der Trainingsdaten wird durch die generierten Daten ausgemacht mit dem Ziel, durch diese ein grundlegendes Verständnis der zu lösenden Aufgabe der Klassifikation von Existenz und Feldfarbe sowie der Regression von Positionen der Dartpfeilspitzen zu erlangen. Das Salting durch eine geringe Menge echter Daten dient der Festigung der erlernten Grundprinzipien und der Adaption auf echte Daten zur Minimierung des Risikos des Overfittings. Daten, die zum Salting verwendet werden, besitzen zur Regulierung von Kardinalitätsunterschieden eine höhere Gewichtung als generierte Daten.

Supervised Training setzt die Existenz sowohl von Inputs als auch korrekten Outputs voraus, um die getätigten Vorhersagen des Netzwerks auf ihre Korrektheit zu überprüfen und die Netzwerkparameter durch Backpropagation zu adaptieren. Dazu sind einheitliche In- und Outputs notwendig. Die Inputdaten bestehen aus normalisierten 3-Kanal-Farbbildern mit dem Farbformat BGR und Abmessungen von $800 \times 800\,\text{px}$\footnote{Da es sich bei dem trainierten Netzwerk um ein Fully Convolutional Neural Network handelt, ist eine Festlegung auf konkrete Bilddimensionen nicht notwendig. Zur Vereinheitlichung der Daten und zur Normalisierung der Eingaben wurde jedoch eine feste Größe verwendet.}.

Die Outputdaten besitzen die Form $25 \times 25 \times 8 \times 3$. Das Input-Bild wird in $25 \times 25$ Regionen -- entsprechend $32 \times 32\,\text{px}$ je Region --, für die je eine Matrix der Größe $8 \times 3$ vorhergesagt wird. Diese Matrix enthält Informationen zu Existenz von Dartpfeilspitzen in ihrer Region, die relative Position dieser sowie die getroffene Feldfarbe. Je Region können bis zu 3 Dartpfeile identifiziert werden. Eine schematische Veranschaulichung der Datenstruktur ist in \autoref{img:datenformat} gezeigt.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/data_outputs.pdf}
    \caption{Schematische Darstellung des Output-Datenformats. Die Anzahl der Output-Zellen wurde hinsichtlich der Übersichtlichkeit auf $10 \times 10$ Regionen begrenzt.}
    \label{img:datenformat}
\end{figure}

\subsubsection{Validierungsdaten}
\label{sec:validierungsdaten}

Zusätzlich zu den Trainingsdaten werden für das Supervised Training Validierungsdaten verwendet. Diese setzen sich ebenfalls aus unterschiedlichen Quellen zusammen, jedoch in gleichen Anteilen und resultierend ohne spezifische Gewichtung. Analog zu den Trainingsdaten stammen die Validierungsdaten aus einem Pool generierter Daten, Daten des DeepDarts-Datensatzes und manuell aufgenommen und annotierten Daten. Dabei wurde auf eine strikte Trennung der Daten geachtet, sodass die generierten Daten gesondert gerendert, die DeepDarts-Daten aus einem separaten Teil de Daten mit sich unterscheidender Dartscheibe ausgewählt und die manuell aufgenommen Daten an einem anderen Ort aufgenommen wurden als die in den Trainingsdaten vorhandenen Daten. Auf diese Weise ist eine strikte Trennung der Trainings- und Validierungsdaten gegeben, durch die Verzerrungen durch Ähnlichkeiten zwischen Datensätzen minimiert werden während gleichzeitig unterschiedliche Quellen verwendet werden.


\subsubsection{Oversampling}
\label{sec:oversampling}

Die Datenerstellung beruht auf der Verwendung von Heatmaps zur Positionierung der Dartpfeile auf der Dartscheibe, wodurch eine annähernd uniforme Verteilung über die gesamte Dartscheibe resultiert. Durch die Geometrie der Dartscheibe führt diese Art der Verteilung zu einer Verteilungsungleichheit der Klassen schwarzer und weißer Felder im Vergleich zu roten und grünen Feldern. Um diesem Klassenungleichgewicht entgegenzuwirken, wurde ein Oversampling roter und grüner Felder sowie ihrer unmittelbaren Umgebungen durchgeführt.

Durch das Oversampling ist ein übermäßiger Prozentsatz unterrepräsentierter Klassen in den Daten vorhanden, der zu einem ausgeglichenen Lernen aller Klassen führt \cite{oversampling}. Ohne Oversampling flächenmäßig kleiner Felder der Dartscheibe in den Trainingsdaten resultierte die Vorhersage von Double-, Triple- oder Bull-Pfeilen in Fehlklassifizierungen als schwarze bzw. weiße Felder.

Trotz der Vorteile von Oversampling muss auf eine korrekte Einbindung dessen geachtet werden, um eine übermäßige Verzerrung der Datenlage zu verhindern \cite{oversampling_bad}. Für diese Thesis wurden $24\,576$ Daten erstellt, davon $20\,480$ reguläre Daten und $4096$ Oversampling-Daten. Der Prozentsatz des Oversamplings beläuft sich damit auf $16.67\%$ der generierten Daten.

Die Heatmaps zum Generieren der für das Oversampling genutzten Wahrscheinlichkeitsverteilungen sind in \autoref{img:heatmaps} dargestellt; Details zur Datenerstellung mittels dieser Heatmaps wurden in \autoref{sec:wie_dartpfeil_positionen} (\nameref{sec:wie_dartpfeil_positionen}) erläutert.


\subsubsection{Augmentierung}
\label{sec:daten_augmentierung}

Erstrebenswert für die Erstellung von Trainingsdaten ist eine maximale Abdeckung aller plausibler und zu erwartender Parameter, die die Daten ausmachen. Insbesondere bei komplexen Daten -- darunter auch Bilddaten -- ist eine uniforme Abdeckung der Quellmenge an Input-Daten nicht möglich. Es ist daher davon auszugehen, dass die Trainingsdaten mit einer gewissen Verzerrung einhergehen. Durch Augmentierung werden neue Daten aus bereits vorhandenen Daten abgeleitet, indem die Parameter der Daten manipuliert werden. Durch diese Technik wird eine größere Bandbreite möglicher Parameter und resultierend mögliche Eingaben in das System abgedeckt \cite{augmentierung, augmentation_max_likelihood_est}. Durch Verwendung von Datenaugmentierung ist eine generelle Verbesserung der Netzwerkperformance zu erwarten \cite{augmentierung_auswirking}.

Für die Augmentierung von Bilddaten existieren unterschiedliche Herangehensweisen \cite{augmentierung_techniken}. Konkret wurden in dieser Arbeit zwei Arten von Augmentierung auf die Trainingsdaten angewendet: Pixel-Manipulation und Transformation. Die Pipeline zur Augmentierung der Trainingsdaten ist in \autoref{img:augmentierungs_pipeline} dargestellt.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/augmentation_pipeline.pdf}
    \caption{Pipeline der Daten-Augmentierung.}
    \label{img:augmentierungs_pipeline}
\end{figure}

\paragraph{Pixel-Manipulation}

Unter Pixel-Manipulation versteht sich die Änderung von Pixel-Werten der Eingabebilder, die keine Beeinflussung der Output-Daten mit sich ziehen. Diese Manipulation ist in 5 Unterschritte aufgeteilt, die sukzessiv auf die Input-Daten angewandt werden. Die Manipulation beginnt mit einer Aufhellung oder Verdunklung einzelner Farbkanäle der Bilddaten, gefolgt von zufälliger Helligkeits- und Kontrastanpassung. Danach werden die Bilddaten mit einem normalverteilten Gaußschem Rauschen angereichert und zuletzt wird die Sättigung des resultierenden Bildes zufällig gesetzt.

Das Analogon dieser Manipulation ist das Vorherrschen unterschiedlicher Beleuchtungen und Kamera-Parameter bezüglich ISO, Kontrastverhalten und Über- und Unterbelichtung, sowie variierende Qualität der Kameraaufnahmen. Es ist klar zu erkennen, dass diese Art der Augmentierung keinen Einfluss auf die Outputs der Daten nimmt, da lediglich die Repräsentation der Pixel, jedoch nicht die Aussage des Bildes abgeändert wird.

\paragraph{Transformation}

Entgegen der Pixel-Manipulation nimmt die Transformation Einfluss auf die Output-Daten, indem affine Transformationen auf die Daten angewandt und somit die Positionen der Dartpfeilspitzen in den Bildern manipuliert werden. Die Transformation besteht aus drei Schritten: Flipping, Rotation und Translation. Bei dem Flipping wird das Bild zufällig horizontal und vertikal gespiegelt. Die Rotation rotiert das Bild um Ganzzahlige Vielfache von $18\degree$ um den Mittelpunkt. Diese Rotationsinkremente entsprechen den Feldlinienwinkeln der Dartscheibe und sorgen dafür, dass die Wertigkeit der Dartfelder rotiert, jedoch die Position der Feldlinien erhalten bleibt, da diese durch die in \autoref{cha:cv} vorgestellte Vorverarbeitung vorgegeben ist. Zuletzt wird das Bild durch eine Translation in $x$- und $y$-Richtung um wenige Pixel verschoben, um Ungenauigkeiten des Normalisierungsalgorithmus zu simulieren.

\vspace{\baselineskip}

Diese Augmentierungen werden dynamisch beim Einlesen der Daten angewandt, sodass jedes Sample in jeder Epoche des Trainings unterschiedlich augmentiert wird. Durch diese Technik wird Overfitting durch Auswendiglernen der Daten entgegengewirkt und die Fähigkeit des neuronalen Netzes zur Generalisierung der Problemstellung wird verstärkt. Das Augmentieren ermöglicht das Fokussieren auf die Extraktion relevanter Kerninformationen in den Daten anstelle oberflächlicher Erscheinungsbilder.

\subsubsection{Dynamisches Training}
\label{sec:dynamisches_training}

Die Geschwindigkeit des Trainings wird durch die Learning Rate gesteuert. Diese beschränkt den Grad des Einflusses der durch die Backpropagation ermittelten Änderungen auf die Netzwerkparameter. Eine hohe Learning Rate sorgt für rapide und starke Änderungen während eine geringe Learning Rate eine geringe Änderung und langsame Konvergenz der Parameterwerte mit sich zieht. Für das Training in dieser Thesis wurde eine adaptive Learning Rate verwendet, die dynamisch auf den Verlauf des Trainings reagiert und für eine gezielte Konvergenz sorgt \cite{adaptive_lr_schedule}.

Es wurde ein Learning Rate Schedule gewählt, der die Learning Rate zu Beginn des Trainings ansteigen lässt und sie heruntersetzt, sobald über eine vorbestimmte Dauer keine Verbesserung des Validation-Losses geschehen ist:

\begin{equation*}
    \text{lr}(e) =
    \begin{cases}
        \frac{\text{lr}_0}{e+1},                                      & \text{wenn\,} e < e_\text{warmup},                                                               \\
        \min(\text{lr}_\text{min}, \text{lr}(e-1) \cdot f_\text{lr}), & \text{wenn\,} \min(\text{val\_loss}) \notin \{\text{val\_loss}_i \mid i \in [0,e-w_\text{lr}]\}, \\
        \text{lr}(e-1),                                               & \text{ansonsten.}
    \end{cases}
\end{equation*}
\nomenclature{$e \in \mathbb{N}$}{Aktuelle Trainingsepoche}
\nomenclature{$e_\text{warmup} \in \mathbb{N}$}{Anzahl an Warmup-Epochen für das Training}
\nomenclature{$\text{lr}_0$}{Initiale Learning Rate}
\nomenclature{$\text{lr}_\text{min}$}{Minimale Learning Rate}
\nomenclature{$f_\text{lr}$}{Adaptionsfaktor der Learning Rate}
\nomenclature{$w_\text{lr}$}{Fenstergröße des Learning Rate-Schedules}

Es wurden Parameterbelegungen $\text{lr}_0 = 0.001$, $\text{lr}_\text{min} = 10^{-6}$, $e_\text{warmup} = 8$, $f_\text{lr} = 0.1^{0.25}$ und $w_\text{lr} = 50$ gewählt. Der Wert von $f$ wurde derart gesetzt, dass die Learning Rate nach 4-facher Verringerung um einen Faktor 10 gemindert wurde. Mit den gewählten Werten wird die Anzahl der Verringerungen der Learning Rate im Verlaufe eines gesamten Trainings auf 12 festgelegt, sofern das Training nicht zuvor abgebrochen wird.

\subsubsection{Trainingsablauf}

Für das Training wurden die erwähnten Techniken und Optimierungen eingesetzt. Als Optimizer wurde sich für AdamW entschieden, der sich durch adaptive Gradientenanpassung auszeichnet und empirisch für solide Generalisierungsfähigkeiten bekannt ist \cite{adamw, adamw_good, adamw_good2, adamw_good3, adamw_good4}.

\todo{Daten zum Training, Graphen etc - EDIT: Das ist teils in Implementierung schon. Überdenken!}

% -------------------------------------------------------------------------------------------------

