% !TeX root = ../main.tex

\section{Methodik}
\label{sec:ki:methodik}

Im Anschluss an die Grundlagen widmet sich dieser Abschnitt der Methodik hinter der Konzeption und der Herangehensweise der Lokalisierung von Dartpfeilen durch ein neuronales Netz. In einem ersten Unterabschnitt wird der Aufbau und das Konzept der verwendeten Netzwerkarchitektur beschrieben. Insbesondere wird auf die Hintergründe und Besonderheiten der Netzwerkarchitektur eingegangen. Danach folgt eine Betrachtung der verwendeten Loss-Funktion für das Training. Diese ist spezifisch auf die Netzwerkarchitektur zugeschnitten, sodass ein optimales Training ermöglicht wird. Zuletzt wird das Netzwerktraining thematisiert. Es wird auf die Wahl und Zusammensetzung der Trainings- und Validierungsdaten eingegangen, Parameter werden erläutert und der Verlauf des Trainings wird dargestellt.

% -------------------------------------------------------------------------------------------------

\subsection{Die verwendete Architektur}
\label{sec:warum_yolov8}

Die Vorhersagen der Dartpfeilpositionen in normalisierten Bildern geschieht durch ein neuronales Netz der YOLOv8*-Architektur. Diese Architektur basiert auf YOLOv8 \cite{yolov8_paper}, welche für den Einsatz der Dartpfeilerkennung umstrukturiert wurde.

Dieses Unterkapitel thematisiert zunächst die Hintergründe der Wahl von YOLOv8 als Basismodell in \autoref{sec:hintergrund_yolov8}. Danach werden in \autoref{sec:yolo_adaption} vorgenommene Adaptionen an der Architektur aufgezeigt. Im Anschluss wird der konkrete Aufbau von YOLOv8* in \autoref{sec:yolov8_aufbau} dargestellt.

\subsubsection{Hintergründe zur Wahl von YOLOv8 als Basismodell}
\label{sec:hintergrund_yolov8}

Die Architektur von YOLOv8 ist derart parametrisiert, dass unterschiedliche Netzwerkgrößen als Varianten vorgegeben sind. Die Größe und damit einhergehend die Komplexität des Netzwerks ist aufgeteilt in die Klassen n (nano), s (small), m (medium), l (large) und x (extra large). Diese Varianten unterscheiden sich in den Größen und Anzahlen der verwendeten Schichten und sind für unterschiedliche Einsatzsituationen ausgelegt. Während l- und x-Modelle für Umgebungen ausgelegt sind, in denen vorhandene Rechenleistung keinen Engpass darstellt, sind die s- und n-Modelle für den Einsatz in mobilen Geräten oder Edge-Devices vorgesehen, in denen die Ressourcen begrenzt sind. Die Verwendung von Netzwerken geringerer Größen geht mit Einbußen in der Qualität der Vorhersagen einher.

Zusätzlich zu den genannten Charakteristiken ist YOLOv8 ein optimierter Nachfolger der für DeepDarts verwendeten YOLOv4-Architektur. Durch den Einsatz von YOLOv4 in DeepDarts konnte die Fähigkeit dieser Familie der Netzwerkarchitekturen hinsichtlich der Erkennung von Dartpfeilen gezeigt werden.

Aufgrund des vorgesehenen Einsatzbereichs des Systems dieser Arbeit in mobilen Endgeräten ist die flexible Netzwerkgröße in Kombination mit einer hohen Qualität der Vorhersagen ausschlaggebend für die Entscheidung, diese Netzwerkarchitektur als Basismodell für die Ausarbeitung dieser Arbeit zu verwenden.

\vspace*{-0.12cm}
\subsubsection{Adaption des Modells}
\label{sec:yolo_adaption}

Obgleich durch \citeauthor{deepdarts} der Einsatz von YOLO-Architekturen zur Identifizierung von Dartpfeilen gezeigt wurde, ist die generelle Strukturierung der Architektur nicht optimal für die zugrundeliegende Aufgabe. Für eine Abstimmung der Architektur auf die Aufgabe werden strukturelle Änderungen im Netzwerkaufbau unternommen. Die adaptierte Netzwerkarchitektur wird im Folgenden als YOLOv8* bezeichnet, um eine Differenzierung zu der offiziellen YOLOv8-Architektur herzustellen.

\vspace*{-0.12cm}
\paragraph{Bounding Boxes}

Im Wesentlichen ist die Verwendung von Ankerpunkten mit umliegenden Bounding Boxes bei der Lokalisierung von Objekten in Bildern effektiv und zielführend. Bei der Vorhersage spezifischer Positionen in einem Bild liefert die Verwendung von Bounding Boxes jedoch keinen Vorteil. Das DeepDarts-System projiziert quadratische Bounding Boxes auf die Dartpfeilspitzen, um Outputdaten zum Training des Netzwerks zu generieren. Die Positionsberechnung der identifizierten Dartpfeile bezieht die Bounding Boxes nicht mit ein, da der im Mittelpunkt der quadratischen Bounding Box liegende Ankerpunkt die Position des Dartpfeils angibt. Die YOLOv8*-Architektur wird an ihren Einsatz angepasst, indem keine Vorhersagen über die Ausdehnung von Bounding Boxes von der Architektur getroffen werden.

\vspace*{-0.12cm}
\paragraph{Multi-Scale-Output}

In der YOLOv8-Architektur wird ein Multi-Scale-Output zur Identifizierung von Objekten unterschiedlicher Größen eingesetzt. Objekte werden in drei unterschiedlichen Kontextgrößen identifiziert; durch Nachverarbeitungsschritte werden diese Outputs miteinander kombiniert. Dieser Multi-Scale-Output ist in der YOLOv8*-Architektur nicht vorhanden, da für die zu identifizierenden Objekte durch die Normalisierung der Bilddaten keine starken Größenvariationen zu erwarten sind. Zudem unterliegt eine kleine Kontextgröße der Gefahr der fehlerhaften Klassifizierung von Abnutzungen der Dartscheibe als Dartpfeil. Der Kontext des gesamten Dartpfeils ist zur Identifizierung seines Einstichpunktes notwendig; dieser ist durch die Normalisierung der Dartscheibe auf eine feste Größe von $800 \times 800\,\text{px}$ durch den größten Kontext gegeben.

\vspace*{-0.12cm}
\paragraph{Dreiteilungen der Outputs}

Die YOLOv8*-Architektur unterteilt das Eingabebild in Regionen und bestimmt die Existenz von Dartpfeilen je Region. Da eine typische Runde Darts aus drei Würfen besteht, ist davon auszugehen, dass eine maximale Anzahl von drei Dartpfeilen in den Bildern zu identifizieren ist. Dabei kann jedoch nicht ausgeschlossen werden, dass sich die Dartpfeilspitzen in unterschiedlichen Regionen befinden. Je Region wird daher eine feste Anzahl von drei möglichen Dartpfeilpositionen vorhergesagt. Existiert lediglich ein Dartpfeil, so sind zwei mögliche Outputs genullt.

\vspace*{-0.12cm}
\paragraph{Transition-Blocks}

Eine grundlegende Änderung der YOLOv8-Architektur ist das Hinzufügen von Transition-Blocks. Diese befinden sich an den Übergängen zwischen Backbone und Head sowie Head und Detect. Sie brechen die Natur des \ac{fcnn} der YOLO-Netzwerke durch Einbindung von Dense-Schichten. Die Eigenschaft von \acp{fcnn}, Bilder beliebiger Eingabegrößen verarbeiten zu können, geht mit der Einschränkung einher, keinen globalen Kontext des Bildes direkt einzufangen. Da die Eingabegrößen der Bilder durch die Vorverarbeitung vorgegeben sind, liefert die Verarbeitung beliebiger Eingabegrößen keinen Mehrwert und ermöglicht damit die Lockerung dieses Paradigmas. Folglich ist eine Einbindung von Dense-Schichten architektonisch möglich und ermöglicht einen globalen Überblick über das Eingabebild, anhand derer Informationen der gesamten Dartscheibe in untergeordnete Abschnitte des Netzwerks einfließen können. Die Einschränkung auf lokale Kontextfenster wird dadurch in der YOLOv8*-Architektur aufgehoben.

\subsubsection{Aufbau der Architektur}
\label{sec:yolov8_aufbau}

Der Aufbau der in dieser Thesis verwendeten YOLOv8*-Architektur ist in \autoref{img:yolov8_architektur} dargestellt. Sie ist unterteilt in die Bereiche Backbone, Head und Detect. Das Backbone und der Head sind weitestgehend analog zur YOLOv8-Architektur strukturiert. Der Detection ist ein gesonderter Netzwerkabschnitt zugeteilt, in welchem die Eliminierung des Multi-Scale-Outputs sowie die gesonderte Handhabung der Outputs manifestiert ist. Sie ist dazu unterteilt in drei parallele Stränge, in welchen Auswertungen zu Existenz, Position und Klasse abgeleitet und zu einem gemeinsamen Output konkateniert werden. Dieser Output besitzt eine Größe von $25 \times 25 \times 8 \times 3$, wie in \autoref{img:datenformat} dargestellt.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/methodik/yolov8.pdf}
    \caption{YOLOv8*-Architektur. (1) Bottleneck; Extraktion von Features. (2) Head; Kombination von Features. (3) Detect; Deutung von Features. Inspiriert durch \citeauthor{yolo_model_visualization_inspration} \cite{yolo_model_visualization_inspration}}
    \label{img:yolov8_architektur}
\end{figure}

Die Netzwerkarchitektur setzt sich aus unterschiedlichen Blöcken zusammen. Grundlegende Blöcke sind: zweidimensionale Faltung (Conv2d), Max-Pooling (MaxPool2d), Addition (Add), zweidimensionale Normalisierung (BatchNorm2d), SiLU-Aktivierungsfunktion (SiLU) sowie Zweiteilung von Feature-Maps (Split) und Konkatenation von Feature Maps (Concat). Zusätzlich zu diesen bereits in YOLOv8 vorhandenen Grundblöcken werden Dense-Schichten (Dense), Dropout-Schichten (Dropout) und explizite Formänderungen der Tensoren (Reshape) ergänzt.

Aus diesen Grundblöcken werden weitere, größere Netzwerkblöcke zusammengesetzt, aus der die YOLOv8*-Architektur aufgebaut ist. Bereits in YOLOv8 enthaltene Blöcke sind SPPF, C2f und Bottleneck mit und ohne Shortcut. Adaptiert ist der Conv-Block, indem eine Dropout-Schicht hinzugefügt wurde. Als neuer Block ist der Transition-Block aufgenommen worden, der ähnlich zu dem Bottleneck-Block mit Shortcut aufgebaut ist, jedoch mit der Addition einer Dense-Schicht. Die zusammengesetzten Blöcke der YOLOv8*-Architektur sind in \autoref{img:yolov8_parts} dargestellt.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/methodik/yolov8_parts.pdf}
    \caption{Netzwerkbestandteile der YOLOv8*-Architektur. (1) SPPF-Block; dieser zeichnet sich durch Hintereinanderreihung von MaxPool2d-Schichten aus. (2) C2f-Block; dieser spaltet die Feature Maps auf und wendet wiederholt Bottleneck-Blöcke an. (3, 4) Bottleneck-Block ohne und mit Shortcut; wiederholte Anwendung von Convolution, ggf. mit residualem Shortcut. (5) Conv-Block; dieser stellt den Grundbaustein der Convolution dar. (6) Transition-Block; residuale Dense-Schicht zum Einfangen globaler Informationen.}
    \label{img:yolov8_parts}
\end{figure}

\vspace*{-0.1cm}
\subsubsection{Verwendete Konfiguration von YOLOv8*}
\label{sec:yolov8_konfiguration}

\begin{table}[t]
    \begin{tabular}{c||c|c|c}
        Konfiguration & Anzahl der Schichten & Trainierbare Parameter & Gesamtparameter \\ \hline
        n             & 188                  & 3,21 M                 & 3,23 M          \\
        s             & 188                  & 10,48 M                & 10,51 M         \\
        m             & 234                  & 18,24 M                & 18,27 M         \\
        l             & 280                  & 25,06 M                & 25,10 M         \\
        x             & 280                  & 38,70 M                & 38,75 M
    \end{tabular}
    \caption{Parameter- und Schichtzahlen unterschiedlicher Konfigurationen der YOLOv8*-Architektur.}
    \label{tab:yolov8_params}
\end{table}

Die variable Größe von YOLOv8 ist analog in YOLOv8* übernommen worden, sodass die Konfigurationen n, s, m, l und x möglich sind. Die verwendete Architektur dieser Thesis orientiert sich in ihrer Größenordnung an der in DeepDarts verwendeten Architektur YOLOv4-tiny, welche ca. $6$ Millionen Parameter beinhaltete. Größenordnungen der Parametrisierungen sind in \autoref{tab:yolov8_params} dargestellt. Aufgrund der im Vergleich zu DeepDarts komplexeren Daten wird sich für die Verwendung von YOLOv8*-s entschieden, welche mit etwa $10,\!5$ Millionen Parametern etwa $40\,\%$ mehr Parameter besitzt als YOLOv4-tiny.

% -------------------------------------------------------------------------------------------------

\vspace*{-0.2cm}
\subsection{Loss-Funktionen}
\label{sec:losses}

Die Loss-Funktion, die für das Training von YOLOv8* verwendet wird, ist nach dem Vorbild bereits existierender Loss-Funktionen für Netzwerke der YOLO-Familie konstruiert. Die Loss-Funktion setzt sich aus unterschiedlichen Teil-Losses zusammen, die zu einem gemeinsamen Loss kombiniert werden. Der Aufbau des gemeinsamen Losses durch die untergeordneten Losses folgt dabei der Architektur des Netzwerks.

\vspace*{-0.1cm}
\subsubsection{Zusammensetzung des Losses}

Die Loss-Funktion folgt dem Aufbau der YOLOv8*-Architektur. Sie ist definiert als gewichtete Summe aus Existenz-Loss, Klassen-Loss und Positions-Loss:
\[ \mathcal{L}(y, \hat{y}) = \omega_\text{xst}\,\mathcal{L}_\text{xst}(y, \hat{y}) + \omega_\text{cls}\,\mathcal{L}_\text{cls}(y, \hat{y}) + \omega_\text{pos}\,\mathcal{L}_\text{pos}(y, \hat{y}) \]

\vspace*{-0.4cm}
\paragraph{Existenz-Loss $\mathcal{L}_\text{xst}$}

Für den Existenz-Loss $\mathcal{L}_\text{xst}$ wird der Focal-Loss der Existenz-Einträge aller Einträge jeglicher Regionen gebildet \cite{focal_loss}. Die Verwendung des Focal-Loss für diesen Loss liegt in der spärlichen Menge positiver Einträge in Output-Tensoren. Bei einer Auflösung von $25 \times 25$ Regionen und drei Vorhersagen je Region besitzt der Output-Tensor $n_\text{entries} = 25 \times 25 \times 3 = 1.875$ Einträge. Zu erwarten sind $ n_\text{positive} \leq 3 $ positive Einträge, sodass der maximal zu erwartende Prozentsatz positiver Einträge $p_\text{positive} = \frac{n_\text{positive}}{n_\text{entries}} \leq \frac{1}{625} = 0,\!16\,\%$ beträgt. Der Focal-Loss gewichtet positive und negative Klassen, sodass ein gezieltes Erlernen trotz signifikanten Klassenungleichgewichts möglich ist.

\vspace*{-0.1cm}
\paragraph{Klassen-Loss $\mathcal{L}_\text{cls}$}

Der Klassen-Loss $\mathcal{L}_\text{cls}$ beruht ebenso wie der Existenz-Loss auf dem Focal-Loss. Zur Berechnung des Losses werden die Regionen in Betracht gezogen, die einen Dartpfeil enthalten. Regionen ohne Dartpfeil werden nicht betrachtet, da diesen keine eindeutige Klasse zugeordnet werden kann. Es wird die kategorische Focal-Kreuzentropie (Categorical Focal Cross-Entropy) \cite{focal_loss} verwendet, um den Loss der für die Dartpfeile zugeteilten Klassen und den vorhergesagten Klassen in den jeweiligen Ziel-Regionen zu bestimmen. Die Klassen stehen für die Farben schwarz, weiß, rot und grün sowie einer Klasse zur Identifizierung des Außenbereichs, in dem keine Punkte erzielt werden (vgl. \autoref{img:datenformat}).

\paragraph{Positions-Loss $\mathcal{L}_\text{pos}$}

Die Bestimmung des Positions-Losses $\mathcal{L}_\text{pos}$ geschieht ebenso wie die Bestimmung von $\mathcal{L}_\text{cls}$ unter Berücksichtigung der annotierten Existenzen. Je Region werden Positionen normalisiert relativ zur oberen linken Ecke angegeben. Die Differenz der normalisierten, lokalen Positionen der Vorhersagen und Wahrheitswerte werden in je $x$- und $y$-Komponente berechnet und aufsummiert. Diese kombinierte Summe wird durch die Anzahl der vorhandenen Dartpfeile geteilt, um einen Mittelwert der Positionsabweichung zu berechnen. Existieren keine Positionen, gilt $\mathcal{L}_\text{pos} = 0$.

\paragraph{Gewichtung}

Durch die Verwendung unterschiedlicher Losses für Existenz, Klasse und Position resultieren unterschiedliche Ausgabewerte der Losses. Die Loss-Gewichte $\omega_\text{xst}$, $\omega_\text{cls}$ und $\omega_\text{pos}$ gewichten die Loss-Outputs derart, dass kein Loss wesentlich überwiegt und eine Reduktion des Losses $\mathcal{L}$ eine uniforme Reduktion der Losses $\mathcal{L}_\text{xst}$, $\mathcal{L}_\text{cls}$ und $\mathcal{L}_\text{pos}$ mit sich zieht. Zum Anpassen der Losses aneinander für eine gleichwertige Konvergenz werden die gewichte $\omega_\text{xst} = 400$, $\omega_\text{cls} = 2.000$ und $\omega_\text{pos} = 0,\!5$ verwendet.

\subsubsection{Hintergründe und Zielsetzung}

Der Hintergrund der Aufteilung der Loss-Funktion in Existenz, Klasse und Position liegt in der Netzwerkarchitektur. Diese ist auf die Vorhersage von Existenz, Klasse und Position ausgelegt, um Dartpfeile für ein Scoring zu lokalisieren. Die Kombination unterschiedlicher Losses mit eigenen Gewichten für je einen thematischen Bereich des Netzwerks ermöglicht ein ausgeglichenes und kontextbezogenes Training des gesamten Netzwerks. Auf diese Weise wird der Überschattung eines Teil-Losses durch einen anderen mit weitaus größerem Wert entgegengewirkt. Ein Overfitting eines Bereichs ist dadurch in dem kombinierten Loss eher widergespiegelt, sodass darauf dynamisch während des Trainings eingegangen werden kann.

\subsubsection{Abweichung vom DIoU-Loss}

Bei dem Training von YOLO-Architekturen ist die Verwendung vom IoU-Losses oder Adaptionen wie CIoU, DIoU oder GIoU \cite{diou_losses}, eine übliche Praxis \cite{yolov1,yolov8_paper,yolo_training_giou}. Diese Erweiterungen des IoU-Losses stützen sich grundlegend auf der Annahme der Existenz von Bounding Boxes. Da diese nicht in der YOLOv8*-Architektur vorhanden sind, ist die Verwendung von IoU-basierten Loss-Funktionen obsolet. Es wurde jedoch mit einer Adaption des GIoU-Losses experimentiert, in der Quadrate vordefinierter Größe auf die vorhergesagten Positionen projiziert werden, anhand derer ein IoU-Loss berechnet werden kann. Durch diese Herangehensweise können viele Annahmen getroffen werden, durch die Optimierungen bezüglich Äquivalenz von GIoU-Loss und DIoU-Loss und effiziente Berechnung der Intersection-Area durch Distanzen der Punkte voneinander möglich sind. Da dieser Ansatz jedoch trotz Optimierungen mit einer großen Rechenleistung und starker Kongruenz zum Positions-Loss einherging, wurde diese Idee für das Training des Netzwerks verworfen.

% -------------------------------------------------------------------------------------------------

\subsection{Training}
\label{sec:nn_training}

Das Training des neuronalen Netzes zielt auf das Erlernen der Identifizierung von Dartpfeilspitzen in normalisierten Eingabebildern ab. Das Training stützt sich dabei auf die Verwendung synthetisch generierter Trainingsdaten, die durch sporadische Anreicherung durch reale Daten erweitert und durch starke Augmentierungstechniken vervielfältigt werden. Der Aufbau der Trainingsdaten, die Arten und die Verwendung von Augmentierung sowie der Trainingsablauf werden in den folgenden Unterkapiteln thematisiert.

\subsubsection{Trainingsdaten}
\label{sec:trainingsdaten}

Für das Training des DeepDarts-Systems wurden wenig diverse Daten verwendet, wodurch die Performance des Systems beeinträchtigt ist, wie bereits in \autoref{sec:cv:ergebnisse} dargestellt. Um den einseitigen und wenig diversen Daten entgegenzuwirken, wird in dieser Thesis auf die Nutzung eigener, synthetisch generierter Daten gesetzt, die durch Salting realer Daten angereichert werden. Die Datenerstellung erfolgt nach dem in \autoref{cha:daten} beschriebenen Prinzip.

Die überwiegende Mehrheit der Trainingsdaten wird durch generierte Daten ausgemacht, mit dem Ziel, durch diese ein grundlegendes Verständnis der zu lösenden Aufgabe der Klassifikation von Existenz und Feldfarbe sowie der Regression von Positionen der Dartpfeilspitzen zu erlangen. Durch ein Salting mit realen Daten wird die Fähigkeit des Erlernens von Gegebenheiten realer Bilder ermöglicht. Die Datenquellen dafür setzen sich zusammen aus DeepDarts-Trainingsdaten und realen Aufnahmen, die für das Training dieser Arbeit aufgenommen und annotiert wurden. Daten, die zum Salting verwendet werden, besitzen zur Regulierung von Kardinalitätsunterschieden eine höhere Gewichtung als synthetische Daten.

Ein Training durch Supervised Learning setzt sowohl Inputs wie auch korrekte Outputs voraus, um die getätigten Vorhersagen des Netzwerks auf ihre Korrektheit zu überprüfen und die Netzwerkparameter durch Backpropagation zu adaptieren. Dazu sind einheitliche In- und Outputs notwendig. Die Inputdaten bestehen aus normalisierten 3-Kanal-Farbbildern im GBR-Farbformat mit Abmessungen von $800 \times 800\,\text{px}$.

Die Outputdaten besitzen die Form $25 \times 25 \times 8 \times 3$. Das Input-Bild wird in $25 \times 25$ Regionen -- entsprechend $32 \times 32\,\text{px}$ je Region --, für die je eine Matrix der Größe $8 \times 3$ vorhergesagt wird. Diese Matrix enthält Informationen zu Existenz von Dartpfeilspitzen der ihr zugewiesenen Region, die relative Position dieser sowie die getroffene Feldfarbe. Je Region können bis zu drei Dartpfeile identifiziert werden. Eine schematische Veranschaulichung der Datenstruktur wird in \autoref{img:datenformat} dargestellt.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/methodik/data_outputs.pdf}
    \caption{Schematische Darstellung des Output-Datenformats. Die Anzahl der Output-Zellen ist hinsichtlich der Übersichtlichkeit auf $10 \times 10$ Regionen begrenzt.}
    \label{img:datenformat}
\end{figure}

\subsubsection{Validierungsdaten}
\label{sec:validierungsdaten}

Zusätzlich zu den Trainingsdaten werden für das Training Validierungsdaten verwendet. Diese setzen sich ebenfalls aus unterschiedlichen Quellen zusammen, jedoch in gleichen Anteilen und resultierend ohne spezifische Gewichtung. Analog zu den Trainingsdaten stammen die Validierungsdaten aus einem Pool generierter Daten, Daten des DeepDarts-Datensatzes und manuell aufgenommen und annotierten Daten. Dabei wird auf eine strikte Trennung der Daten geachtet, sodass die generierten Daten gesondert gerendert, die DeepDarts-Daten aus einem separaten Teil der Daten mit sich unterscheidender Dartscheibe ausgewählt und die manuell aufgenommen Daten an einem anderen Ort aufgenommen wurden als die in dem Trainingssatz vorhandenen Daten. Auf diese Weise ist eine strikte Trennung der Trainings- und Validierungsdaten gegeben, durch die Verzerrungen durch Ähnlichkeiten zwischen Datensätzen minimiert werden während gleichzeitig unterschiedliche Quellen verwendet werden.

\subsubsection{Oversampling}
\label{sec:oversampling}

Die Datenerstellung beruht auf der Verwendung von Heatmaps zur Positionierung der Dartpfeile auf der Dartscheibe, wodurch eine annähernd uniforme Verteilung über die gesamte Dartscheibe resultiert. Durch die Geometrie der Dartscheibe führt diese Art der Verteilung zu einer Ungleichheit in der Verteilung der Klassen schwarzer und weißer Felder im Vergleich zu roten und grünen Feldern. Um diesem Klassenungleichgewicht entgegenzuwirken, wird ein Oversampling roter und grüner Felder sowie ihrer unmittelbaren Umgebungen durchgeführt.

Durch das Oversampling ist ein übermäßiger Prozentsatz unterrepräsentierter Klassen in den Daten vorhanden, der zu einem ausgeglichenen Lernen aller Klassen führt \cite{oversampling}. Ohne Oversampling flächenmäßig kleiner Felder resultierten Vorhersagen von Double-, Triple- oder Bull-Treffern vermehrt in Fehlklassifizierungen als schwarze bzw. weiße Felder.

Trotz der Vorteile von Oversampling muss auf eine korrekte Einbindung dessen geachtet werden, um eine übermäßige Verzerrung der Datenlage zu verhindern \cite{oversampling_bad}. Für diese Thesis wurden $24.576$ Daten synthetisch erstellt, davon $20.480$ reguläre Daten und $4.096$ Oversampling-Daten. Der Prozentsatz des Oversamplings beläuft sich damit auf $16,\!67\,\%$ der generierten Daten.

% Die Heatmaps zum Generieren der für das Oversampling genutzten Wahrscheinlichkeitsverteilungen sind in \autoref{img:heatmaps} dargestellt; Details zur Datenerstellung mittels dieser Heatmaps sind in \autoref{sec:wie_dartpfeil_positionen} (\nameref{sec:wie_dartpfeil_positionen}) erläutert.7

\subsubsection{Augmentierung}
\label{sec:daten_augmentierung}

Erstrebenswert für die Erstellung von Trainingsdaten ist eine maximale Abdeckung aller plausibler und zu erwartender Parameter, die die Daten ausmachen. Insbesondere bei komplexen Daten -- darunter auch Bilddaten -- ist eine uniforme Abdeckung der Quellmenge an Input-Daten nicht möglich. Es ist daher davon auszugehen, dass die Trainingsdaten mit einer gewissen Verzerrung einhergehen.

Für die Augmentierung von Bilddaten existieren unterschiedliche Herangehensweisen \cite{augmentierung_techniken}. Konkret werden in dieser Arbeit zwei Arten von Augmentierung auf die Trainingsdaten angewendet: Pixel-Manipulation und Transformation. Die Pipeline zur Augmentierung der Trainingsdaten ist in \autoref{img:augmentierungs_pipeline} dargestellt.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/methodik/augmentation_pipeline.pdf}
    \caption{Schematische Darstellung der Augmentierung von Trainingsdaten.}
    \label{img:augmentierungs_pipeline}
\end{figure}

\paragraph{Pixel-Manipulation}

Pixel-Manipulation bezeichnet die Änderung von Pixel-Werten der Eingabebilder, die keine Beeinflussung der Outputs mit sich zieht. Die Herangehensweise der eingesetzten Pixel-Manipulation ist in fünf Schritte aufgeteilt, die sukzessiv auf die Input-Daten angewandt werden. Die Manipulation beginnt mit einer Aufhellung oder Verdunklung einzelner Farbkanäle der Bilddaten, gefolgt von einer zufälligen Anpassung von Helligkeit und Kontrast. Danach werden die Bilddaten mit einem normalverteilten Gaußschem Rauschen angereichert und zuletzt wird die Sättigung des resultierenden Bildes randomisiert.

Das Analogon dieser Manipulation ist das Vorherrschen unterschiedlicher Beleuchtungen und Kamera-Parameter bezüglich ISO, Kontrastverhalten und Über- und Unterbelichtung, sowie variierende Qualität der Kameraaufnahmen. Es ist klar zu erkennen, dass diese Art der Augmentierung keinen Einfluss auf die Outputs der Daten nimmt, da lediglich die Repräsentation der Pixel, jedoch nicht die Aussage des Bildes abgeändert wird.

\paragraph{Transformation}

Entgegen der Pixel-Manipulation nimmt die Augmentierung durch Transformation Einfluss auf die Output-Daten, indem affine Transformationen auf die Daten angewandt und somit die Positionen der Dartpfeilspitzen in den Bildern manipuliert werden. Die Augmentierung durch Transformation besteht aus drei Schritten: Flipping, Rotation und Translation. Bei dem Flipping wird das Bild zufällig horizontal und vertikal gespiegelt. Bei der Rotation wird das Bild um Ganzzahlige Vielfache von $18\degree$ um den Mittelpunkt rotiert. Diese Rotationsinkremente entsprechen den Feldlinienwinkeln der Dartscheibe und sorgen dafür, dass die Wertigkeit der Dartfelder rotiert, jedoch die Positionen der Feldlinien erhalten bleiben, welche durch die in \autoref{cha:cv} vorgestellte Vorverarbeitung festgesetzt sind. Zuletzt wird das Bild durch eine Translation in $x$- und $y$-Richtung um wenige Pixel verschoben, um Ungenauigkeiten des Normalisierungsalgorithmus zu simulieren.

\vspace{\baselineskip}

\noindent Diese Augmentierungen werden dynamisch beim Einlesen der Daten angewandt, sodass jedes Sample in jeder Epoche des Trainings unterschiedlich augmentiert wird. Durch diese Technik wird Overfitting durch Auswendiglernen der Daten entgegengewirkt und die Fähigkeit des neuronalen Netzes zur Generalisierung der Problemstellung wird verstärkt. Das Augmentieren ermöglicht das Fokussieren auf die Extraktion relevanter Kerninformationen in den Daten anstelle oberflächlicher Erscheinungsbilder.

\subsubsection{Dynamisches Training}
\label{sec:dynamisches_training}

Die Geschwindigkeit des Trainings wird durch die Learning-Rate gesteuert. Diese beschränkt den Grad des Einflusses der durch die Backpropagation ermittelten Änderungen auf die Netzwerkparameter. Eine hohe Learning-Rate sorgt für rapide und starke Änderungen, während eine geringe Learning-Rate eine geringe Änderung und langsame Konvergenz der Netzwerkparameter mit sich zieht. Für das Training in dieser Thesis wurde eine dynamische Learning-Rate verwendet, die basierend auf dem Verlauf des Trainings gesetzt wurde und auf eine gezielte Konvergenz ausgelegt ist \cite{adaptive_lr_schedule}.

Anstelle eines vorprogrammierten Schedules einer Learning-Rate wurde sich vor dem Hintergrund des optimalen Lernerfolgs für eine manuelle Anpassung der Learning-Rate entschieden. Initial wurde die Learning-Rate auf den Wert $\text{lr}_0 = 0,\!001$ gesetzt. Sofern der Trainingserfolg nach subjektiver Einschätzung zu stagnieren begann, wurde die Learning-Rate um den Faktor $f_\text{lr} = 0,\!1^{0,25}$ angepasst. Durch diesen Wert erfolgt eine Minderung der Learning-Rate um den Faktor 10 nach vierfacher Verringerung.

% Es wurde ein Learning-Rate Schedule gewählt, der die Learning-Rate zu Beginn des Trainings ansteigen lässt und sie heruntersetzt, sobald über eine vorbestimmte Dauer keine Verbesserung des Validation-Losses geschehen ist:

% \begin{equation*}
%     \text{lr}(e) =
%     \begin{cases}
%         \frac{\text{lr}_0}{e+1},                                      & \text{wenn\,} e < e_\text{warmup},                                                               \\
%         \min(\text{lr}_\text{min}, \text{lr}(e-1) \cdot f_\text{lr}), & \text{wenn\,} \min(\text{val\_loss}) \notin \{\text{val\_loss}_i \mid i \in [0,e-w_\text{lr}]\}, \\
%         \text{lr}(e-1),                                               & \text{ansonsten.}
%     \end{cases}
% \end{equation*}
% \nomenclature{$e \in \mathbb{N}$}{Aktuelle Trainingsepoche}
% \nomenclature{$e_\text{warmup} \in \mathbb{N}$}{Anzahl an Warmup-Epochen für das Training}
\nomenclature{$\text{lr}_0$}{Initiale Learning-Rate}
% \nomenclature{$\text{lr}_\text{min}$}{Minimale Learning-Rate}
\nomenclature{$f_\text{lr}$}{Adaptionsfaktor der Learning-Rate}
% \nomenclature{$w_\text{lr}$}{Fenstergröße des Learning-Rate-Schedules}

% Es wurden Parameterbelegungen $\text{lr}_0 = 0,\!001$, $\text{lr}_\text{min} = 10^{-6}$, $e_\text{warmup} = 8$, $f_\text{lr} = 0,\!1^{0,\!25}$ und $w_\text{lr} = 50$ gewählt. Der Wert von $f$ wurde derart gesetzt, dass die Learning-Rate nach 4-facher Verringerung um einen Faktor 10 gemindert wurde. Mit den gewählten Werten wird die Anzahl der Verringerungen der Learning-Rate im Verlaufe eines gesamten Trainings auf 12 festgelegt, sofern das Training nicht zuvor abgebrochen wird.

\subsubsection{Trainingsablauf}
\label{sec:trainingsverlauf}

Für das Training wurden die erwähnten Techniken und Optimierungen eingesetzt. Als Optimizer wurde sich für AdamW entschieden, der sich durch adaptive Gradientenanpassung auszeichnet und empirisch für solide Generalisierungsfähigkeiten bekannt ist \cite{adamw, adamw_good, adamw_good2, adamw_good3, adamw_good4}. Zudem konnte bereits gezeigt werden, dass die YOLOv8-Architektur erfolgreich mit diesem Optimizer trainiert werden konnte \cite{adamw_yolo}. Das Training verlief über $500$ Epochen und der Verlauf ist in \autoref{img:trainingsverlauf} dargestellt. Die Abbildung ist unterteilt in die Teil-Losses der Existenz, Klassen und Positionen sowie den kombinierten Loss. Trainings- und Validierungs-Loss fallen tendenziell gemeinsam, jedoch ist eine größere Diskrepanz der jeweiligen Werte bei $\mathcal{L}_\text{xst}$ zu verzeichnen als bei $\mathcal{L}_\text{cls}$ und $\mathcal{L}_\text{pos}$. Die Verläufe der Validierungs-Losses deuten jedoch weder auf Over- noch auf starkes Underfitting hin.

In den Graphen ist ein Sprung der Losses um Epoche $400$ zu vermerken, welcher durch die manuelle Adaption der Learning-Rate bedingt ist. Um einem lokalen Minimum des Losses entgegenzuwirken, wurde die Learning-Rate erhöht und nach einigen Epochen inkrementell verringert. Diese Technik ist als Warm-Restart bekannt und kann zu Verbesserungen der Losses führen \cite{lr_warm_restart}. In diesem Fall konnte eine Verbesserung des Losses durch den Einsatz dieser Technik erzielt werden.

\begin{figure}
    \centering
    \begin{subfigure}{\linewidth}
        \raggedleft
        \includegraphics[height=4.75cm]{imgs/ai/ergebnisse/loss_xst.pdf}
        \hspace{2.3cm}
        \caption{Gewichteter Existenz-Loss $\omega_\text{xst}\,\mathcal{L}_\text{xst}$}
    \end{subfigure}
    \vspace{0.1cm}
    \par

    \begin{subfigure}{\linewidth}
        \raggedleft
        \includegraphics[height=4.75cm]{imgs/ai/ergebnisse/loss_cls.pdf}
        \hspace{2.3cm}
        \caption{Gewichteter Klassen-Loss $\omega_\text{cls}\,\mathcal{L}_\text{cls}$}
    \end{subfigure}
    \vspace{0.1cm}
    \par

    \begin{subfigure}{\linewidth}
        \raggedleft
        \includegraphics[height=4.75cm]{imgs/ai/ergebnisse/loss_pos.pdf}
        \hspace{2.3cm}
        \caption{Gewichteter Positions-Loss $\omega_\text{pos}\,\mathcal{L}_\text{pos}$}
    \end{subfigure}
    \vspace{0.1cm}
    \par

    \begin{subfigure}{\linewidth}
        \raggedleft
        \includegraphics[height=4.75cm]{imgs/ai/ergebnisse/loss.pdf}
        \hspace{2.3cm}
        \caption{Kombinierter Loss $\mathcal{L}$}
    \end{subfigure}

    \caption{Trainingsverlauf der YOLOv8*-Architektur. Die Loss-Werte sind als Punkte dargestellt, welche durch eine Linie der entsprechenden Farbe geglättet dargestellt sind. Trainings-Losses werden blau dargestellt, Validierungs-Losses orange.}
    \label{img:trainingsverlauf}
\end{figure}


% -------------------------------------------------------------------------------------------------

