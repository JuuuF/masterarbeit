% !TEX root = ../main.tex

\section{Methodik}
\label{sec:ki:methodik}

Methodik hier.

% -------------------------------------------------------------------------------------------------

\subsection{YOLOv8}
\label{sec:warum_yolov8}

\subsubsection{Warum dieses Modell?}

Die YOLOv8-Architektur wurde analog zur restlichen Familie der YOLO-Netzwerke für effiziente und genaue Identifizierung von Objekten in Bilddaten entwickelt \cite{yolov8_paper,adamw_yolo}. Zudem werden YOLO-Architekturen in unterschiedlichen Konfigurationen bereitgestellt, die dir Komplexität der Modelle bestimmen. So wurden für die YOLOv8-Architektur die Konfigurationen n (nano), s (small), m (medium), l (large) und x (extra large) bereitgestellt, wobei die Nano-Varianten die geringste Parameterzahl besitzen und die Extra-Large-Varianten die größte. Die Trainingserfolge sind typischerweise proportional zur Parameterzahl der Netzwerke.

\todo{Dieser Absatz ist Grütze.}

YOLOv8 ist ein State-of-the-Art Single-Shot CNN, welches in der Lage ist, selbst unter Verwendung weniger Parameter und in ressourcenbegrenzten Umgebungen in Echtzeit verwendet zu werden. Darüber hinaus wurde mit YOLOv4 die Vorgängerversion dieser Netzwerkarchitektur im DeepDarts-System zur Vorhersage von Dartpfeilpositionen verwendet. Durch die Arbeit des DeepDarts-Systems wurde die Fähigkeit des Erlernens dieser Aufgabe durch Netzwerke der YOLO-Familie bereits erarbeitet und es kann in dieser Thesis auf diesem Wissen aufgebaut werden.

\subsubsection{Adaption des Modells}
\label{sec:yolo_adaption}

Obgleich durch \citeauthor{deepdarts} der Einsatz von YOLO-Architekturen zur Identifizierung von Dartpfeilen gezeigt wurde ist die generelle Strukturierung der Architektur nicht optimal für die zugrundeliegende Aufgabe. Für eine Abstimmung von Architektur und Aufgabe wurden strukturelle Änderungen im Netzwerkaufbau unternommen. Die Adaptierte Netzwerkarchitektur wird im Folgenden als YOLOv8* bezeichnet, um eine Differenzierung zwischen zu der offiziellen YOLOv8-Architektur herzustellen.

\paragraph{Bounding Boxes}

Im Wesentlichen ist die Verwendung von Ankerpunkten mit umliegenden Bounding Boxes bei der Lokalisierung von Objekten in Bildern effektiv und zielführend. Bei der Vorhersage spezifischer Positionen in einem Bild liefert die Verwendung von Bounding Boxes jedoch keinen signifikanten Vorteil. Das DeepDarts-System projizierte quadratische Bounding Boxes auf die Dartpfeilspitzen, um Outputdaten zum Training des Netzwerks zu generieren. Diese Bounding Boxes wurden im Verlauf des Trainings verkleinert, bis sie eine minimale Größe erreichten \cite{deepdarts}. Die letztendliche Positionsberechnung der identifizierten Dartpfeile bezog die Bounding Boxes nicht mit ein, da der Ankerpunkt der quadratischen Bounding Box die Position des Dartpfeils angibt. Die YOLOv8*-Architektur wurde an ihren Einsatz angepasst, indem die Verwendung von Bounding Boxes aus der Architektur eliminiert wurde.

\paragraph{Multi-Scale-Output}

In der YOLOv8-Architektur wird ein Multi-Scale-Output zur Identifizierung von Objekten unterschiedlicher Größen eingesetzt. Objekte werden in 3 unterschiedlichen Kontextgrößen identifiziert; durch Nachverarbeitungsschritte werden diese Outputs miteinander kombiniert. Dieser Multi-Scale-Output ist in der YOLOv8*-Architektur nicht vorhanden, da für die zu identifizierenden Objekte keine starken Größenvariationen zu erwarten sind. Zudem unterliegt eine kleine Kontextgröße der Gefahr der fehlerhaften Klassifizierung von Abnutzungen der Dartscheibe als Dartpfeil. Der Kontext des gesamten Dartpfeils ist zur Identifizierung seines Einstichpunktes notwendig; dieser ist durch die Normalisierung der Dartscheibe auf eine feste Größe von $800 \times 800\,\text{px}$ durch den größten Kontext gegeben.

\paragraph{Dreiteilungen der Outputs}

Die YOLOv8*-Architektur unterteilt das Eingabebild in Regionen und bestimmt die Existenz von Dartpfeilen je Region. Da eine typische Runde Darts aus 3 Würfen besteht, ist davon auszugehen, dass eine maximale Anzahl von 3 Dartpfeilen in den Bildern zu identifizieren sind. Dabei kann jedoch nicht ausgeschlossen werden, dass sich die Dartpfeilspitzen in unterschiedlichen Regionen befinden. Je Region wird daher eine feste Anzahl von 3 möglichen Dartpfeilpositionen vorhergesagt. Existiert lediglich ein Dartpfeil, so sind zwei mögliche Outputs genullt.

\paragraph{Transition-Block}

Eine grundlegende Änderung der YOLOv8-Architektur ist das Hinzufügen von Transition-Blocks. Diese befinden sich an den Übergängen zwischen Backbone und Head sowie Head und Detect. Sie brechen den Fully-Convolutional-Approach der YOLO-Netzwerke durch Einbindung von Dense-Schichten. Die Eigenschaft von Fully-Connected-CNNs, Bilder beliebiger Eingabegrößen verarbeiten zu können, geht mit der Einschränkung einher, keinen direkten globalen Kontext des Bildes einzufangen. Da die Eingabegrößen der Bilder der in dieser Thesis erarbeiteten Systems durch die Vorverarbeitung vorgegeben sind, liefert die Verarbeitung beliebiger Eingabegrößen keinen Mehrwert und ermöglicht damit die Lockerung dieses Paradigmas. Folglich ist eine Einbindung von Fully-Connected-Schichten architektonisch möglich und ermöglicht einen globalen Überblick über das Eingabebild, anhand derer Informationen der gesamten Dartscheibe in untergeordnete Abschnitte des Netzwerks einfließen können. Die Einschränkung auf lokale Kontextfenster wird dadurch in der YOLOv8*-Architektur aufgehoben.

\subsubsection{Konfiguration des Netzwerks}

Für diese Thesis wurden die Konfigurationen YOLOv8*-n und YOLOv8*-x trainiert, um einen Überblick über die Fähigkeiten der Modellarchitektur zu erlangen. Nano-Varianten der Modelle sind für die Ausführen in Echtzeitumgebungen und in Szenarien mit begrenzter Rechenleistung, beispielsweise in Mobiltelefonen, ausgelegt. Ihre Inferenzzeit ist durch ihre geringe Größe gering im Vergleich zu anderen Konfigurationen. Die Extra-Large-Varianten der Modelle werden für maximale Performance hinsichtlich der Ergebnisse verwendet. Sie werden in Systemen verwendet, in denen Rechenleistung kein Bottleneck darstellt.

\todo{Tatsächliche Konfiguration aufzeigen.}

YOLOv8-n bzw. YOLOv8-x
Konfiguration erklären -> woher kommen die Varianten?
Anzahl Parameter
Vergleich mit DeepDarts-Netzwerk

\subsubsection{Aufbau der Architektur}

Der Aufbau des in dieser Thesis verwendeten YOLOv8*-Architektur ist in \autoref{img:yolov8_architektur} dargestellt. Sie ist unterteilt in die Bereiche Backbone, Head und Detect. Das Backbone und der Head sind im wesentlichen analog zur YOLOv8-Architektur strukturiert. Jedoch wurde die Detection in einen eigenen Bereich überführt und adaptiert indes durch die Eliminierung des Multi-Scale-Outputs lediglich eine Verbindung zwischen Head und Detection besteht. Der Detection-Bereich ist unterteilt in drei parallele Stränge, die Existenz, Position und Klasse vorhersagen und zu einem gemeinsamen Output konkateniert werden. Dieser Output besitzt eine Größe von $25 \times 25 \times 8 \times 3$.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/yolov8.pdf}
    \caption{YOLOv8*-Architektur. (1) Bottleneck; Extraktion von Features. (2) Head; Kombination von Features. (3) Detect; Deutung von Features.}
    \label{img:yolov8_architektur}
\end{figure}

Die Bestandteile der Architektur sind in \autoref{img:yolov8_parts} dargestellt. Es wurden 4 zusammengesetzte Blöcke definiert, die unterliegende Funktionen im Aufbau von YOLOv8* übernehmen.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/yolov8_parts.pdf}
    \caption{Netzwerkbestandteile der YOLOv8*-Architektur. (1) SPPF-Block; dieser zeichnet sich durch Hintereinanderreihung von MaxPool2d-Schichten aus. (2) C2f-Block; dieser spaltet die Feature Maps auf und wendet wiederholt Bottleneck-Blöcke an. (3, 4) Bottleneck-Block ohne und mit Shortcut; wiederholte Anwendung von Convolution, ggf. mit residualem Shortcut. (5) Conv-Block; dieser stellt den Grundbaustein der Convolution dar. (6) Transition-Block; Residualer Fully-Connected layer zum Einfangen globaler Informationen.}
    \label{img:yolov8_parts}
\end{figure}

% -------------------------------------------------------------------------------------------------

\subsection{Loss-Funktionen}
\label{sec:losses}

\todo{Einleitende Sätze}

\subsubsection{Zusammensetzung des Losses}

Die Loss-Funktion zum Trainieren der YOLOv8*-Architektur folgt dem Aufbau dieser. Sie ist definiert als gewichtete Summe aus Existenz-Loss, Klassen-Loss und Positions-Loss:
\[ \mathcal{L}(y, \hat{y}) = \omega_\text{xst} \mathcal{L}_\text{xst}(y, \hat{y}) + \omega_\text{cls} \mathcal{L}_\text{cls}(y, \hat{y}) + \omega_\text{pos} \mathcal{L}_\text{pos}(y, \hat{y}) \]
\paragraph{Existenz-Loss}

Für den Existenz-Loss $\mathcal{L}_\text{xst}$ wird der Focal-Loss der Existenz-Einträge aller Einträge aller Regionen gebildet \cite{focal_loss}. Die Verwendung des Focal-Loss für diesen Loss liegt in der spärlichen Menge positiver Einträge in Output-Tensoren. Bei einer Auflösung von $25 \times 25$ Regionen und 3 Vorhersagen je Region besitzt der Output-Tensor $n_\text{entries} = 25 \times 25 \times 3 = 1875$ Einträge. Zu erwarten sind $ n_\text{positive} \leq 3 $ positive Einträge, sodass der maximal zu erwartende Prozentsatz positiver Einträge $p_\text{positive} = \frac{n_\text{positive}}{n_\text{entries}} \leq \frac{1}{625} = 0.16\% $. Der Focal-Loss gewichtet positive und negative Klassen, sodass ein gezieltes Erlernen trotz signifikanten Klassenungleichgewichts.

\paragraph{Klassen-Loss}

Der Klassen-Loss beruht ebenso wie der Existenz-Loss auf dem Focal-Loss. Zur Berechnung des Losses werden die Regionen in Betracht gezogen, die einen Dartpfeil enthalten. Regionen ohne Dartpfeil werden nicht betrachtet, da diesen keine eindeutige Klasse zugeordnet werden kann. Es wird die kategorische Focal-Kreuzentropie (Categorical Focal Cross-Entropy) \cite{focal_loss} verwendet, um den Loss der den Dartpfeilen zugeordneten Klassen und den vorhergesagten Klassen in den jeweiligen Ziel-Regionen zu bestimmen. Die Klassen stehen für die Farben schwarz, weiß, rot und grün sowie einer Klasse zur Identifizierung des Außenbereichs, in dem keine Punkte erzielt werden (vgl. \autoref{img:datenformat}).

Dies, das, class.

\todo{}

\paragraph{Positions-Loss}

Position, yo ho.

\todo{}

\paragraph{Gewichtung}

Durch die Verwendung unterschiedlicher Losses für Existenz, Klasse und Position resultieren unterschiedliche Bildbereiche der Loss-Werte. Die Loss-Gewichte $\omega_\text{xst}$, $\omega_\text{cls}$ und $\omega_\text{pos}$ gewichten die Loss-Outputs derart, dass kein Loss wesentlich überwiegt und eine Reduktion des Losses $\mathcal{L}$ eine uniforme Reduktion der Losses $\mathcal{L}_\text{xst}$, $\mathcal{L}_\text{cls}$ und $\mathcal{L}_\text{pos}$ mit sich zieht. Zum Anpassen der Losses aneinander für eine gleichwertige Konvergenz wurden die gewichte $\omega_\text{xst} = 400$, $\omega_\text{cls} = 850$ und $\omega_\text{pos} = 0.5$ verwendet.

\subsubsection{Hintergründe und Zielsetzung}

Targeting bestimmter Bereiche des Netzwerks

\todo{}

\subsubsection{Abweichung von DIoU-Loss}

Bei dem Training von YOLO-Architekturen ist die die Verwendung vom IoU-Losses oder Adaptionen wie CIoU, DIoU oder GIoU \cite{diou_losses}, eine übliche Praxis \cite{yolov1,yolov8_paper,yolo_training_giou}. Diese Erweiterungen des IoU-Losses stützen sich grundlegend auf der Annahme der Existenz von Bounding Boxes. Da diese in der YOLOv8*-Architektur eliminiert wurden, ist die Verwendung von IoU-basierten Loss-Funktionen obsolet. Es wurde jedoch mit einer Adaption des GIoU-Losses experimentiert, in der Quadrate vordefinierter Größe auf die vorhergesagten Positionen projiziert werden, anhand derer ein IoU-Loss möglich wäre. Durch diese Herangehensweise können viele Annahmen getroffen werden, durch die Optimierungen bezüglich Äquivalenz von GIoU-Loss und DIoU-Loss und effiziente Berechnung der Intersection-Area durch Distanzen der Punkte voneinander möglich sind. Da dieser Ansatz jedoch trotz Optimierungen mit einer großen Rechenleistung und starker Kongruenz zum Positions-Loss einherging, wurde diese Idee für das Training des Netzwerks verworfen.

% -------------------------------------------------------------------------------------------------

\subsection{Training}
\label{sec:nn_training}

Das Training des neuronalen Netzes zielt auf das Erlernen der Identifizierung von Dartpfeilspitzen in normalisierten Eingabebildern ab. Das Training stützt sich dabei auf die Verwendung synthetisch generierter Trainingsdaten, die durch sporadische Anreicherung durch echte Daten erweitert und durch starke Augmentierungstechniken vervielfältigt werden. Der Aufbau der Trainingsdaten, die Arten und die Verwendung von Augmentierung sowie der Trainingsablauf werden in den folgenden Unterkapiteln thematisiert und im Detail beschrieben.

\subsubsection{Trainingsdaten}
\label{sec:trainingsdaten}

Die Trainingsdaten bilden die Basis des Trainings eines neuronalen Netzes. Für das Training des DeepDarts-Systems wurden wenig diverse Daten verwendet, wodurch die Performance des Systems beeinträchtigt wurde, wie bereits in \autoref{sec:cv:ergebnisse} dargestellt wurde. Um diesem Phänomen der einseitigen und wenig diversen Daten entgegenzuwirken, wurde in dieser Thesis auf die Nutzung eigener, synthetisch generierter Daten gesetzt, die durch Salting echter Daten angereichert wurden. Die Datenerstellung erfolgte nach dem in \autoref{cha:daten} beschriebenen Prinzip. Zum Salting wurden einerseits Daten aus dem DeepDarts-Datensatz verwendet, sowie manuell aufgenommene Daten aus einem Lokal.

Die überwiegende Mehrheit der Trainingsdaten wird durch die generierten Daten ausgemacht mit dem Ziel, durch diese ein grundlegendes Verständnis der zu lösenden Aufgabe der Klassifikation von Existenz und Feldfarbe sowie der Regression von Positionen der Dartpfeilspitzen zu erlangen. Das Salting durch eine geringe Menge echter Daten dient der Festigung der erlernten Grundprinzipien und der Adaption auf echte Daten zur Minimierung des Risikos des Overfittings. Daten, die zum Salting verwendet werden, besitzen zur Regulierung von Kardinalitätsunterschieden eine höhere Gewichtung als generierte Daten.

Supervised Training setzt die Existenz sowohl von Inputs als auch korrekten Outputs voraus, um die getätigten Vorhersagen des Netzwerks auf ihre Korrektheit zu überprüfen und die Netzwerkparameter durch Backpropagation zu adaptieren. Dazu sind einheitliche In- und Outputs notwendig. Die Inputdaten bestehen aus normalisierten 3-Kanal-Farbbildern mit dem Farbformat BGR und Abmessungen von $800 \times 800\,\text{px}$\footnote{Da es sich bei dem trainierten Netzwerk um ein Fully Convolutional Neural Network handelt, ist eine Festlegung auf konkrete Bilddimensionen nicht notwendig. Zur Vereinheitlichung der Daten und zur Normalisierung der Eingaben wurde jedoch eine feste Größe verwendet.}.

Die Outputdaten besitzen die Form $25 \times 25 \times 8 \times 3$. Das Input-Bild wird in $25 \times 25$ Regionen -- entsprechend $32 \times 32\,\text{px}$ je Region --, für die je eine Matrix der Größe $8 \times 3$ vorhergesagt wird. Diese Matrix enthält Informationen zu Existenz von Dartpfeilspitzen in ihrer Region, die relative Position dieser sowie die getroffene Feldfarbe. Je Region können bis zu 3 Dartpfeile identifiziert werden. Eine schematische Veranschaulichung der Datenstruktur ist in \autoref{img:datenformat} gezeigt.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/data_outputs.pdf}
    \caption{Schematische Darstellung des Output-Datenformats. Die Anzahl der Output-Zellen wurde hinsichtlich der Übersichtlichkeit auf $10 \times 10$ Regionen begrenzt.}
    \label{img:datenformat}
\end{figure}

\subsubsection{Validierungsdaten}
\label{sec:validierungsdaten}

Zusätzlich zu den Trainingsdaten werden für das Supervised Training Validierungsdaten verwendet. Diese setzen sich ebenfalls aus unterschiedlichen Quellen zusammen, jedoch in gleichen Anteilen und resultierend ohne spezifische Gewichtung. Analog zu den Trainingsdaten stammen die Validierungsdaten aus einem Pool generierter Daten, Daten des DeepDarts-Datensatzes und manuell aufgenommen und annotierten Daten. Dabei wurde auf eine strikte Trennung der Daten geachtet, sodass die generierten Daten gesondert gerendert, die DeepDarts-Daten aus einem separaten Teil de Daten mit sich unterscheidender Dartscheibe ausgewählt und die manuell aufgenommen Daten an einem anderen Ort aufgenommen wurden als die in den Trainingsdaten vorhandenen Daten. Auf diese Weise ist eine strikte Trennung der Trainings- und Validierungsdaten gegeben, durch die Verzerrungen durch Ähnlichkeiten zwischen Datensätzen minimiert werden während gleichzeitig unterschiedliche Quellen verwendet werden.


\subsubsection{Oversampling}
\label{sec:oversampling}

Die Datenerstellung beruht auf der Verwendung von Heatmaps zur Positionierung der Dartpfeile auf der Dartscheibe, wodurch eine annähernd uniforme Verteilung über die gesamte Dartscheibe resultiert. Durch die Geometrie der Dartscheibe führt diese Art der Verteilung zu einer Verteilungsungleichheit der Klassen schwarzer und weißer Felder im Vergleich zu roten und grünen Feldern. Um diesem Klassenungleichgewicht entgegenzuwirken, wurde ein Oversampling roter und grüner Felder sowie ihrer unmittelbaren Umgebungen durchgeführt.

Durch das Oversampling ist ein übermäßiger Prozentsatz unterrepräsentierter Klassen in den Daten vorhanden, der zu einem ausgeglichenen Lernen aller Klassen führt \cite{oversampling}. Ohne Oversampling flächenmäßig kleiner Felder der Dartscheibe in den Trainingsdaten resultierte die Vorhersage von Double-, Triple- oder Bull-Pfeilen in Fehlklassifizierungen als schwarze bzw. weiße Felder.

Trotz der Vorteile von Oversampling muss auf eine korrekte Einbindung dessen geachtet werden, um eine übermäßige Verzerrung der Datenlage zu verhindern \cite{oversampling_bad}. Für diese Thesis wurden $24\,576$ Daten erstellt, davon $20\,480$ reguläre Daten und $4096$ Oversampling-Daten. Der Prozentsatz des Oversamplings beläuft sich damit auf $16.67\%$ der generierten Daten.

Die Heatmaps zum Generieren der für das Oversampling genutzten Wahrscheinlichkeitsverteilungen sind in \autoref{img:heatmaps} dargestellt; Details zur Datenerstellung mittels dieser Heatmaps wurden in \autoref{sec:wie_dartpfeil_positionen} (\nameref{sec:wie_dartpfeil_positionen}) erläutert.


\subsubsection{Augmentierung}
\label{sec:daten_augmentierung}

Erstrebenswert für die Erstellung von Trainingsdaten ist eine maximale Abdeckung aller plausibler und zu erwartender Parameter, die die Daten ausmachen. Insbesondere bei komplexen Daten -- darunter auch Bilddaten -- ist eine uniforme Abdeckung der Quellmenge an Input-Daten nicht möglich. Es ist daher davon auszugehen, dass die Trainingsdaten mit einer gewissen Verzerrung einhergehen. Durch Augmentierung werden neue Daten aus bereits vorhandenen Daten abgeleitet, indem die Parameter der Daten manipuliert werden. Durch diese Technik wird eine größere Bandbreite möglicher Parameter und resultierend mögliche Eingaben in das System abgedeckt \cite{augmentierung, augmentation_max_likelihood_est}. Durch Verwendung von Datenaugmentierung ist eine generelle Verbesserung der Netzwerkperformance zu erwarten \cite{augmentierung_auswirking}.

Für die Augmentierung von Bilddaten existieren unterschiedliche Herangehensweisen \cite{augmentierung_techniken}. Konkret wurden in dieser Arbeit zwei Arten von Augmentierung auf die Trainingsdaten angewendet: Pixel-Manipulation und Transformation. Die Pipeline zur Augmentierung der Trainingsdaten ist in \autoref{img:augmentierungs_pipeline} dargestellt.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/ai/augmentation_pipeline.pdf}
    \caption{Pipeline der Daten-Augmentierung.}
    \label{img:augmentierungs_pipeline}
\end{figure}

\paragraph{Pixel-Manipulation}

Unter Pixel-Manipulation versteht sich die Änderung von Pixel-Werten der Eingabebilder, die keine Beeinflussung der Output-Daten mit sich ziehen. Diese Manipulation ist in 5 Unterschritte aufgeteilt, die sukzessiv auf die Input-Daten angewandt werden. Die Manipulation beginnt mit einer Aufhellung oder Verdunklung einzelner Farbkanäle der Bilddaten, gefolgt von zufälliger Helligkeits- und Kontrastanpassung. Danach werden die Bilddaten mit einem normalverteilten Gaußschem Rauschen angereichert und zuletzt wird die Sättigung des resultierenden Bildes zufällig gesetzt.

Das Analogon dieser Manipulation ist das Vorherrschen unterschiedlicher Beleuchtungen und Kamera-Parameter bezüglich ISO, Kontrastverhalten und Über- und Unterbelichtung, sowie variierende Qualität der Kameraaufnahmen. Es ist klar zu erkennen, dass diese Art der Augmentierung keinen Einfluss auf die Outputs der Daten nimmt, da lediglich die Repräsentation der Pixel, jedoch nicht die Aussage des Bildes abgeändert wird.

\paragraph{Transformation}

Entgegen der Pixel-Manipulation nimmt die Transformation Einfluss auf die Output-Daten, indem affine Transformationen auf die Daten angewandt und somit die Positionen der Dartpfeilspitzen in den Bildern manipuliert werden. Die Transformation besteht aus drei Schritten: Flipping, Rotation und Translation. Bei dem Flipping wird das Bild zufällig horizontal und vertikal gespiegelt. Die Rotation rotiert das Bild um Ganzzahlige Vielfache von $18\degree$ um den Mittelpunkt. Diese Rotationsinkremente entsprechen den Feldlinienwinkeln der Dartscheibe und sorgen dafür, dass die Wertigkeit der Dartfelder rotiert, jedoch die Position der Feldlinien erhalten bleibt, da diese durch die in \autoref{cha:cv} vorgestellte Vorverarbeitung vorgegeben ist. Zuletzt wird das Bild durch eine Translation in $x$- und $y$-Richtung um wenige Pixel verschoben, um Ungenauigkeiten des Normalisierungsalgorithmus zu simulieren.

\vspace{\baselineskip}

Diese Augmentierungen werden dynamisch beim Einlesen der Daten angewandt, sodass jedes Sample in jeder Epoche des Trainings unterschiedlich augmentiert wird. Durch diese Technik wird Overfitting durch Auswendiglernen der Daten entgegengewirkt und die Fähigkeit des neuronalen Netzes zur Generalisierung der Problemstellung wird verstärkt. Das Augmentieren ermöglicht das Fokussieren auf die Extraktion relevanter Kerninformationen in den Daten anstelle oberflächlicher Erscheinungsbilder.

\subsubsection{Dynamisches Training}

Die Geschwindigkeit des Trainings wird durch die Learning Rate gesteuert. Diese beschränkt den Grad des Einflusses der durch die Backpropagation ermittelten Änderungen auf die Netzwerkparameter. Eine hohe Learning Rate sorgt für rapide und starke Änderungen während eine geringe Learning Rate eine geringe Änderung und langsame Konvergenz der Parameterwerte mit sich zieht. Für das Training in dieser Thesis wurde eine adaptive Learning Rate verwendet, die dynamisch auf den Verlauf des Trainings reagiert und für eine gezielte Konvergenz sorgt \cite{adaptive_lr_schedule}.

Es wurde ein Learning Rate Schedule gewählt, der die Learning Rate zu Beginn des Trainings ansteigen lässt und sie heruntersetzt, sobald über eine vorbestimmte Dauer keine Verbesserung des Validation-Losses geschehen ist:

\begin{equation*}
    \text{lr}(e) =
    \begin{cases}
        \frac{\text{lr}_0}{e+1},                            & \text{wenn\,} e < e_\text{warmup},                                                     \\
        \min(\text{lr}_\text{min}, \text{lr}(e-1) \cdot f_\text{lr}), & \text{wenn\,} \min(\text{val\_loss}) \notin \{\text{val\_loss}_i \mid i \in [0,e-w_\text{lr}]\}, \\
        \text{lr}(e-1),                                     & \text{ansonsten.}
    \end{cases}
\end{equation*}
\nomenclature{$e \in \mathbb{N}$}{Aktuelle Trainingsepoche}
\nomenclature{$e_\text{warmup} \in \mathbb{N}$}{Anzahl an Warmup-Epochen für das Training}
\nomenclature{$\text{lr}_0$}{Initiale Learning Rate}
\nomenclature{$\text{lr}_\text{min}$}{Minimale Learning Rate}
\nomenclature{$f_\text{lr}$}{Adaptionsfaktor der Learning Rate}
\nomenclature{$w_\text{lr}$}{Fenstergröße des Learning Rate-Schedules}

Es wurden Parameterbelegungen $\text{lr}_0 = 0.001$, $\text{lr}_\text{min} = 10^{-6}$, $e_\text{warmup} = 8$, $f_\text{lr} = 0.1^{0.25}$ und $w_\text{lr} = 50$ gewählt. Der Wert von $f$ wurde derart gesetzt, dass die Learning Rate nach 4-facher Verringerung um einen Faktor 10 gemindert wurde. Mit den gewählten Werten wird die Anzahl der Verringerungen der Learning Rate im Verlaufe eines gesamten Trainings auf 12 festgelegt, sofern das Training nicht zuvor abgebrochen wird.

\subsubsection{Trainingsablauf}

Für das Training wurden die erwähnten Techniken und Optimierungen eingesetzt. Als Optimizer wurde sich für AdamW entschieden, der sich durch adaptive Gradientenanpassung auszeichnet und empirisch für solide Generalisierungsfähigkeiten bekannt ist \cite{adamw, adamw_good, adamw_good2, adamw_good3, adamw_good4}.

\todo{Daten zum Training, Graphen etc}

% -------------------------------------------------------------------------------------------------

