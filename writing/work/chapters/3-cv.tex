% !TeX root = ../main.tex

\chapter{Vorverarbeitung von Bildern durch klassische \acl{cv}}
\label{cha:cv}

Sowohl für das Training als auch für die Inferenz werden normalisierte Bilder von dem neuronalen Netz erwartet. Diese Normalisierung kann auf unterschiedliche Arten geschehen. Für DeepDarts wurde sich entschieden, die Normalisierung und Vorhersage der Dartpfeilpositionen in einem Schritt von dem neuronalen Netz vorhersagen zu lassen. Dieser Ansatz geht jedoch mit einigen Schwachpunkten einher. Wesentliche Nachteile werden offensichtlich in Hinsicht auf die Identifizierung spezifischer Fixpunkte, die möglicherweise verdeckt sein könnten, und Variabilität der Trainingsdaten. Das Angehen einer Aufgabe durch ein neuronales Netz stützt sich auf die Korrektheit und einen angemessenen Umfang der für das Training genutzten Daten. Durch gewollte oder ungewollte Einbindung einer verzerrten Datenlage erlernt ein neuronales Netz eben diese Eigenheiten und es besteht die Gefahr des Overfittings, sodass eine Verallgemeinerung der System-Performance auf unbekannte Daten nicht oder nur bedingt möglich ist.

Eine algorithmische Herangehensweise ist im Gegensatz zu einem neuronalen Netz keine Blackbox und die innere Arbeitsweise ist bekannt und klar definiert. Es kann strikt nachvollzogen werden, an welcher Stelle und aus welchem Grund eine fehlerhafte Vorhersage auftritt, und Problemquellen können gezielt angegangen werden. Darüber hinaus kann eine algorithmische Normalisierung von Daten ohne den Mehraufwand eines neuen Trainings eines neuronalen Netzes auf neue Daten erweitert werden. Zur Anpassung des Systems an neue Gegebenheiten sind lediglich wenige exemplarische Bilddaten notwendig, anhand derer charakteristische Eigenschaften abzuleiten sind und durch welche die Arbeitsweise des Algorithmus angepasst werden kann.

Zusätzlich ist die zu lösende Aufgabe der Identifizierung von Dartscheiben durch ihre Geometrie und ihren Aufbau mit kontrastreichen und farblich markanten Feldern prädestiniert für eine algorithmische Verarbeitung. Diese Charakteristiken werden von Algorithmen und Techniken der herkömmlichen \ac{cv} genutzt, um relevante Informationen zu extrahieren.

Aus diesen Gründen wird sich in dieser Thesis für eine algorithmische Normalisierung der Daten entschieden, die nach der Datenerstellung den zweiten Themenbereich ausmacht. In diesem Kapitel werden in einem ersten Schritt die für das Verständnis des Algorithmus notwendigen Grundlagen in \autoref{sec:cv:grundlagen} erläutert. Darauf folgend wird in \autoref{sec:cv:methodik} auf die Methodik der Normalisierung eingegangen und anschließend wird auf einige relevante Themen der Implementierungen eingegangen; \autoref{sec:cv:implementierung}. Zuletzt werden die Ergebnisse dieser Normalisierung anhand idealer Entzerrungen ausgewertet und mit dem Ansatz des DeepDarts-Systems verglichen.

\input{chapters/3-cv-grundlagen.tex}
\input{chapters/3-cv-methodik.tex}
\input{chapters/3-cv-implementierung.tex}
\input{chapters/3-cv-ergebnisse.tex}
