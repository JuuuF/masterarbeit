% !TEX root = ../main.tex

\section{Grundlagen}
\label{sec:ki:grundlagen}

Zum Verständnis dieses Kapitels wird analog zu \autoref{cha:cv} mit Grundlagen zu Konzepten und Begrifflichkeiten begonnen. Diese ermöglichen ein grundlegendes Verständnis, um die in diesem Kapitel eingesetzten Techniken zu verstehen und den weiteren Unterkapiteln folgen zu können.

Begonnen wird mit der Klärung, was neuronale Netze sind und wie sie technisch funktionieren in \autoref{sec:was_nn}. Dabei wird spezifisch auf bestimmte Arten neuronaler Netze und Arten von Vorhersagen, die in dieser Arbeit genutzt werden, eingegangen. Danach folgt ein Überblick über das Training neuronaler Netze in \autoref{sec:was_nn_training} und grundlegende Terminologie in \autoref{sec:nn_terminologie}. Im Anschluss darauf wird der Begriff der Augmentierung in \autoref{sec:was_augmentierung} erklärt und es wird eine für diese Arbeit relevante Netzwerkarchitektur erläutert \autoref{sec:was_yolov8}.

% -------------------------------------------------------------------------------------------------

\subsection{Was sind neuronale Netze?}
\label{sec:was_nn}

Neuronale Netze sind Kern eines spezifischen Bereichs des Machine Learnings, in dem sich auf das Erlernen von Eigenschaften auf Grundlage von Daten fokussiert wird. Durch sie wird die Tür zur Approximation beliebiger Funktionen geöffnet, indem Resultate der zu erlernenden Funktionen gegeben werden \cite{cv_general}. Die Komplexität der Funktionen ist dabei beliebig, sodass die Spanne möglicher Einsatzbereiche von Sinuswellenapproximation bis zur Generierung natürlicher Sprache und Interaktion mit Menschen im Einsatzbereich von neuronalen Netzen liegt. Der für diese Arbeit relevante Teilbereich des Einsatzes neuronaler Netze ist die Extraktion von Informationen aus Bilddaten.

Hauptsächlich ausschlaggebend für den Erfolg des neuronalen Netzes ist seine Architektur, ihr innerer Aufbau. Herkömmliche neuronale Netze werden aus Aneinanderreihung von Schichten aufgebaut, die eingehende Daten transformieren und für die Verarbeitung subsequenter Schichten zur Verfügung stellen. Die Art der Schicht gibt die Spezifikation der Transformationen an, sodass unterschiedliche Schichten die Daten unterschiedlich verarbeiten. Innerhalb dieser Schichten existieren Parameter, die die Arbeitsweise der Transformation steuern.

Durch Training des neuronalen Netzes werden die Parameter der Schichten derart angepasst, dass durch die schichtweise Verarbeitung der Eingabedaten die gewünschten Ergebnisse erzielt werden. Der Name der neuronalen Netze leitet sich von im Gehirn vorzufindenden Neuronen ab, die für den Gedankenfluss verantwortlich sind. Das Erlernen von Parametern zur Steuerung von Ausgaben ist der Arbeitsweise von Neuronen nachempfunden. Der ursprüngliche Kern neuronaler Netze war die technische Replikation der Arbeitsweise von Gehirnen.

\subsubsection{Convolutional Neural Networks (CNNs)}
\label{sec:cnns}

Die Art der verarbeiteten Daten in einem neuronalen Netz kann viele Formen annehmen. Insbesondere die Verarbeitung von Bilddaten ist ein großer Themenbereich neuronaler Netze und essenziell für diese Thesis. Bereits in \autoref{sec:was_filterung} wurde die Faltung auf Bilddaten eingeführt, die auf Grundlage von Kerneln funktioniert. Auf dieser Arbeitsweise fußen die Convolutional-Schichten. Die Parameter dieser Schichten bestimmen die Ausprägung eines Kerbels, der auf eingehende Bilddaten angewandt wird. Durch vielfache Hintereinanderreihung von Convolutional-Schichten können inkrementell komplexere Strukturen in Bildern identifiziert und abstrakt festgehalten werden \cite{alexnet}. Neuronale Netze, die auf Convolutional-Schichten aufbauen, werden als Convolutional Neural Networks, kurz CNNs, bezeichnet \cite{cv_general}.

\subsubsection{Klassifizierung und Regression}
\label{sec:klassifizierung_regression}

Ebenso komplex wie Eingabedaten neuronaler Netze können ihre Ausgaben sein. Allgemein lassen sich Ausgaben von neuronalen Netzen in zwei Kategorien einteilen: Klassifikation und Regression\footnote{Neben Klassifikation und Regression sind weitere Arten von Ausgaben möglich, beispielsweise Embeddings von Autoencodern. Für den Kontext dieser Arbeit sind diese Arten der Ausgaben jedoch nicht relevant, weshalb sich auf die gängigen Ausgaben klassischer neuronaler Netze beschränkt wird.} \cite{nn_terminology,cv_general}.

Bei der Klassifikation werden Datenpunkte in Form von Klassen vorhergesagt. Bei der Klassifizierung von Bildern sind Netzwerkausgaben der Klassen unterschiedlicher Objekte, Lebewesen oder Eigenschaften möglich (Beispielsweise die Beantwortung der Frage \quotes{\textit{Welches} Tier ist in diesem Bild zu sehen?}). Ebenso ist eine binäre Klassifikation hinsichtlich der Existenz bestimmter Sachverhalte üblich (Beispielsweise die Beantwortung der Frage \quotes{\textit{Existiert} eine Katze in diesem Bild?}). Die Ausgabe von Netzwerken geschieht für diese Arten der Fragen typischerweise in Form von Vektoren, die diese Kategorien durch One-Hot-Encoding (oder 1-of-n-Encoding) darstellen \cite{one_hot_encoding}. Dabei ist jeder Kategorie ein Eintrag im Vektor zugeordnet; die Größe der Zahlenwerte geben die Ausgaben des Netzes für die jeweiligen Kategorien an.

Konträr zur Klassifikation diskreter Gegebenheiten ist die Vorhersage kontinuierlicher Werte als Ausgabe eines neuronalen Netzes möglich, die Regression. Beispiele für Ausgaben einer Regression beinhalten Funktionswerte oder Koordinaten. Ziel einer Regression ist es, konkrete Zahlenwerte vorherzusagen. Sofern eine Begrenzung der ausgegebenen Werte möglich ist, ist die Normalisierung von Daten in die Intervalle $[0, 1]$ oder $[-1, 1]$ üblich. Der Hintergrund dieser Normalisierung liegt in der Arbeitsweise der Netzwerkschichten und wird üblicherweise in einem Nachverarbeitungsschritt nach der Vorhersage des neuronalen Netzes wieder umgekehrt.

In dieser Thesis werden sowohl binäre als auch klassenbezogene Klassifikation sowie Regression verwendet. Die binäre Klassifikation wird zur Identifizierung von Dartpfeilen genutzt, klassenbezogene Klassifikation zur Identifizierung von Feldfarben unter Dartpfeilen und Regression wird genutzt, um die exakten Positionen der Dartpfeile auf der Dartscheibe darzustellen.

% -------------------------------------------------------------------------------------------------

\subsection{Training Neuronaler Netze}
\label{sec:was_nn_training}

Das Training neuronaler Netze kann abhängig von seinen Ausgaben auf unterschiedliche Arten verlaufen. In dieser Arbeit wird Supervised Learning verwendet, bei welchem Eingabedaten mit ihren zugehörigen Ausgaben gegeben sind. Das Netzwerk erlernt auf der Grundlage dieser Daten Parameter, durch die die Ausgaben zu den jeweiligen Eingaben ableiten. Weitere Methoden zum Training neuronaler Netze sind Unsupervised Learning und Reinforcement Learning. Bei Unsupervised Learning liegen lediglich Eingabedaten vor und die Ausgaben werden von dem Netzwerk identifiziert. Diese Art des Lernens wird beispielsweise bei Clustering von Datenpunkten oder Findung von Wort-Embeddings verwendet. Reinforcement Learning wird genutzt, um einem System das Agieren in einer Umgebung zu ermöglichen und basiert auf Belohnung gewünschter Ereignisse und Bestrafung nicht gewünschter Ereignisse \cite{nn_terminology}. Im Kontext dieser Arbeit lediglich Supervised Learning angewendet, um das neuronale Netz zu trainieren.

Supervised Training basiert auf der Korrektur von Fehlern getätigter Vorhersagen des neuronalen Netzes. Als Forward Pass eines neuronalen Netzes wird die Vorhersage von Daten bezeichnet, durch die eine Ausgabe des Netzes erzeugt wird. Der Fehler von Vorhersagen wird durch eine Metrik gemessen, die hinsichtlich der Parameter des Netzwerks differenzierbar ist. Diese Metrik wird als Loss-Funktion\footnote{Alternativ wird diese Funktion auch als Cost- oder Error-Funktion bezeichnet. In dieser Arbeit wird die Terminologie der Loss-Funktion verwendet.} bezeichnet, der Fehler des Netzwerks als Loss. Durch die Differenzierbarkeit ist ihr Gradient bekannt und kann genutzt werden, um lokale Minima zu identifizieren. Je geringer der Fehler ist, desto korrekter sind die Vorhersagen des Systems. Das Erreichen eines Minimums der Loss-Funktion ist das Ziel des Trainings. Die Identifizierung der Parameterangleichungen zur Annäherung an ein Minimum der Loss-Funktion geschieht durch einen Prozess, der als Backpropagation bezeichnet wird. Während der Backpropagation werden iterativ Parameter der Netzwerkschichten angepasst, um die Vorhersage für die gegebenen Daten derart zu korrigieren, dass ein folgender Forward Pass auf den selben Daten einen geringeren Loss mit sich zieht \cite{cv_general}.

Die Implementierung der Backpropagation und die Umsetzung der Parameteranpassung geschieht durch Optimierungsalgorithmen. Die Arbeitsweise dieser Algorithmen ist grundlegend ähnlich hinsichtlich der Eingaben des Problems, in den konkreten Arbeitsweisen unterscheiden sie sich jedoch. Die Auswahl eines passenden Optimierungsalgorithmus ist abhängig von der zugrundeliegenden Aufgabe und der Netzwerkarchitektur.

% -------------------------------------------------------------------------------------------------

\subsection{Terminologie}
\label{sec:nn_terminologie}

Der Themenbereich der neuronalen Netze umfasst eine Vielzahl von Konzepten und Begriffen. Dieses Unterkapitel gibt einen Überblick über die zentralen Begriffe, die in dieser Arbeit von Bedeutung sind \cite{nn_terminology}.

\paragraph{Trainingsdaten}

Dreh- und Angelpunkt des Trainings neuronaler Netze sind die Trainingsdaten. Sie werden genutzt, um die Parameter des zu trainierenden neuronalen Netzes anzupassen, indem getroffene Vorhersagen bewertet werden. Mit dieser Fehlerbewertung durch die Loss-Funktion werden die Parameter während der Backpropagation angepasst. Trainingsdaten haben üblicherweise die größte Kardinalität aller für das Training und die Evaluation verwendeten Datensätze.

Um ein effektives Training eines neuronalen Netzes zu gewährleiten, ist die Wahl der Trainingsdaten essenziell. Da der Trainingserfolg eines neuronalen Netzes abhängig von den Trainingsdaten ist und die durch die Parameter erlernten Strukturen in den Daten nicht bekannt sind, ist eine möglichst uniforme Abdeckung der zugrundeliegenden Daten wichtig. Jegliche Verzerrungen der Datenlage wird potenziell von dem neuronalen Netz erlernt und kann zu einer fehlerhaften Inferenz auf neuen Daten führen.

\paragraph{Validierungsdaten}

Validierungsdaten werden ebenso wie die Trainingsdaten während des Trainings eines neuronalen Netzes genutzt, konträr zu Trainingsdaten haben diese jedoch keinen Einfluss auf den Trainingserfolg. In regelmäßigen Intervallen wird der aktuelle Stand der Netzwerkparameter auf den Validierungsdaten ausgewertet, um Einblicke in die Performance des Netzwerks auf Daten zu gewinnen, die nicht für das Training verwendet wurden. Durch diese Daten können Rückschlüsse auf die Fähigkeit des neuronalen Netzes gezogen werden, das Gelernte auf neue Daten zu übertragen und die zugrundeliegenden Strukturen der Daten zu generalisieren. Die strikte Separierung von Trainings- und Validierungsdaten ist dabei obligatorisch, um eine Verzerrung der Generalisierbarkeit zu vermeiden \cite{nn_terminology}.

Die Wahl der Validierungsdaten unterliegt den gleichen Voraussetzungen wie den Trainingsdaten, um Verzerrungen in die Einblicke der Netzwerk-Performance zu vermeiden. Darüber hinaus sollten Validierungsdaten jedoch auf eine Art und Weise gewählt werden, die nicht zu große Ähnlichkeiten zu den Trainingsdaten aufweist, da diese Nähe der Daten ebenfalls eine Verzerrung der Datenlage mit sich ziehen kann, sofern keine uniforme Verteilung der Trainingsdaten vorliegt.

\paragraph{Testdaten}

Nach dem Training eines neuronalen Netzes werden Testdaten genutzt, um die Netzwerk-Performance zu evaluieren. Während Trainings- und Validierungsdaten Einfluss auf den Verlauf des Trainings nehmen, werden Testdaten genutzt, um einen unabhängigen Einblick in die Netzwerkperformance nach Beendigung des Trainings zu gewinnen \cite{nn_terminology}. Durch Testdaten wird die Inferenz des trainierten Netzes auf unbekannten Daten simuliert, wodurch eine objektive Abschätzung der Generalisierbarkeit ermöglicht wird.

Für die Wahl der Testdaten sind die selben Voraussetzungen zu beachten, die für Trainings- und Validierungsdaten gelten, um einer Verzerrung der Datenlage vorzubeugen. Die Wahl von Trainings-, Validierungs- und Testdaten spielt für die Auswertung des neuronalen Netzes dieser Thesis eine wichtige Rolle.

\paragraph{\Acl{ood}-Training}

Dass die für das Trainings verwendeten Daten einen universellen Überblick über die gesamte Datenlage geben, ist häufig nicht möglich. Verzerrungen der Datenlage sind -- gewollt oder ungewollt -- in den meisten Fällen nicht zu umgehen. Weichen die Trainingsdaten jedoch bewusst von den Validierungs- und Testdaten ab, spricht man von \Acf{ood}-Training. Das neuronale Netz wird auf Daten trainiert, die daher einer anderen Verteilung entsprechen als der zu erwartenden Daten für die Inferenz des Netzes.

\paragraph{Under- und Overfitting}

Die Validierungsdaten eines Trainings werden verwendet, um den Erfolg eines Trainings zu beurteilen. Während Optimierungsalgorithmen darauf ausgelegt sind, den Trainings-Loss zu minimieren, ist es möglich, dass der Validierungs-Loss von diesem abweicht. Befindet sich der Wert des Validierungs-Loss signifikant über dem Trainings-Loss, spricht man von Underfitting \cite{nn_terminology,cv_general}. In dieser Situation ist das neuronale Netz nicht in der Lage, das Gelernte auf neue Daten anzuwenden, da es noch nicht ausreichend trainiert wurde. Fallen Trainings- und Validierungs-Loss zeitweise gleichermaßen, gefolgt von einem Anstieg des Validierungs-Losses, wird dies als Overfitting bezeichnet \cite{cv_general}. In dieser Situation werden Vorhersagen auf den Trainingsdaten besser, jedoch verliert das neuronale Netz die Fähigkeit der Generalisierbarkeit gelernter Strukturen auf neue Daten. Es werden nicht mehr die relevanten Aussagen hinter den Daten erlernt, sondern die konkreten Ausprägungen in den Trainingsdaten.

% -------------------------------------------------------------------------------------------------

\subsection{Was ist Augmentierung?}
\label{sec:was_augmentierung}

Für ein robustes Training eines neuronalen Netzes und für die Vermeidung von Overfitting ist eine große und möglichst umfangreiche Datenlage notwendig. Zur künstlichen Vervielfältigung der für das Training vorhandenen Daten kann eine Technik, die als Augmentierung bekannt ist, genutzt werden \cite{cv_general}. Als Augmentierung wird eine Manipulation der Trainingsdaten beschrieben, bei der die Datenmenge vervielfältigt wird, ohne die Integrität der Daten zu beeinträchtigen. 

Beispiele für Augmentierung von Bilddaten sind die Anwendung affiner Transformationen oder das Hinzufügen von Rauschen. Sofern die Magnitude dieser Manipulationen nicht derart groß ist, dass die relevanten Aussagen der Bilddaten unterdrückt werden, ist die Erschaffung neuer Grunddaten aus Bildern mit bekannten Annotationen möglich. Ein robust trainiertes neuronales Netz ist in der Lage, die relevanten Informationen aus den Bildern zu extrahieren und eine Invarianz gegenüber der in der Augmentierung angewandten Operationen zu entwickeln. Bei dem Overfitting eines Netzwerks ist diese Invarianz gegenüber der Augmentierungsoperationen nicht gegeben.

% -------------------------------------------------------------------------------------------------

\subsection{Die YOLOv8-Architektur}
\label{sec:was_yolov8}

YOLOv8 ist eine Netzwerkarchitektur, die 2023 durch Ultralytics außerhalb des Rahmens einer gesonderten wissenschaftlichen Aufarbeitung veröffentlicht wurde\footnote{Siehe: \url{https://github.com/ultralytics/ultralytics/issues/2572}}.Mit YOLOv8 wurde eine Netzwerkarchitektur vorgestellt, die Objekterkennung durch Anchor-Free-Erkennungsmechanismen in Echtzeit und in hoher Genauigkeit ermöglicht \cite{yolov1_to_yolov8}. Sie wurde als Verbesserung der YOLOv5-Architektur vorgestellt und ist für den Einsatz in Edge-Devices optimiert \cite{yolov8_comparison}.

Der Aufbau von YOLOv8 folgt einem Design, welches in Backbone, Neck und Head unterteilt ist. Als Backbone wird eine erweiterte Version des CSPDarknet-Backbones verwendet während derNeck auf dem \ac{pan} basiert. Mit dem Aufbau von YOLOv8 wird das Fully-Convolutional-Paradigma neuronaler Netze verfolgt, welches die Verarbeitung von Bilddaten beliebiger Dimensionen ermöglicht.

Die Ausgaben des Netzwerks verfolgen einen Multi-Scale-Ansatz, durch welchen die Erkennung von Objekten unterschiedlicher Größen durch Rasterung der Eingabebilder in unterschiedliche Größen geschieht. Das Bild wird in Zellen unterschiedlicher Skalierungen unterteilt und es werden Vorhersagen für jede Zelle hinsichtlich Mittelpunkt eines Objekts, seiner Klasse und der Ausdehnung des Objekte über das Bild durch die Angabe einer Bounding Box. Die Ausgaben des Netzwerks werden durch etablierte \ac{nms} nachverarbeitet.

Der Einsatzbereich von YOLOv8 spezifiziert sich auf Echtzeitanwendungen, in welchen robuste und akkurate Objekterkennung notwendig ist und in welchen die zur Verfügung stehenden Ressourcen für das Deployment dieses Netzwerks begrenzt sind. Beispiele solcher Anwendungen sind der Einsatz in autonomen Fahrzeugen unt der Objekterkennung in Drohnen \cite{yolov8_car,yolov8_drone}.

% -------------------------------------------------------------------------------------------------
