% !TeX root = ../main.tex

\section{Grundlagen}
\label{sec:ki:grundlagen}

Zum Verständnis dieses Kapitels wird mit Grundlagen zu Konzepten und Begrifflichkeiten begonnen. Diese ermöglichen ein grundlegendes Verständnis, um die in diesem Kapitel eingesetzten Techniken nachzuvollziehen und den weiteren Unterkapiteln folgen zu können.

Begonnen wird mit der Klärung, was neuronale Netze sind und wie sie technisch funktionieren in \autoref{sec:was_nn}. Dabei wird spezifisch auf bestimmte Arten neuronaler Netze und Arten von Vorhersagen, die in dieser Arbeit genutzt werden, eingegangen. Danach folgt ein Überblick über das Training neuronaler Netze in \autoref{sec:was_nn_training} und grundlegende Terminologie in \autoref{sec:nn_terminologie}. Im Anschluss darauf wird der Begriff der Augmentierung in \autoref{sec:was_augmentierung} erklärt und es wird eine für diese Arbeit relevante Netzwerkarchitektur in \autoref{sec:was_yolov8} erläutert.

% -------------------------------------------------------------------------------------------------

\subsection{Was sind neuronale Netze?}
\label{sec:was_nn}

Neuronale Netze sind Gegenstand eines spezifischen Bereichs des Machine Learnings, in dem sich auf das Erlernen von Verteilungen auf Grundlage von Daten fokussiert wird. Durch sie wird die Tür zur Approximation beliebiger Funktionen geöffnet, indem Resultate der zu erlernenden Funktionen gegeben werden \cite{cv_general}. Die Komplexität der Funktionen ist dabei beliebig, sodass die Spanne möglicher Einsatzbereiche von Sinuswellenapproximation bis zur Generierung natürlicher Sprache und Interaktion mit Menschen im Einsatzbereich von neuronalen Netzen liegt. Der für diese Arbeit relevante Teilbereich des Einsatzes neuronaler Netze ist die Extraktion von Informationen aus Bilddaten.

Hauptsächlich ausschlaggebend für den Erfolg des neuronalen Netzes ist seine Architektur, ihr innerer Aufbau. Herkömmliche neuronale Netze werden aus Aneinanderreihung von Schichten aufgebaut, die eingehende Daten transformieren und für die Verarbeitung subsequenter Schichten zur Verfügung stellen. Die Art der Schicht gibt die Spezifikation der Transformationen an, sodass unterschiedliche Schichten die Daten unterschiedlich verarbeiten. Innerhalb dieser Schichten existieren Parameter, die die Arbeitsweise der Transformation steuern. Durch Training des neuronalen Netzes werden die Parameter der Schichten derart angepasst, dass durch die schichtweise Verarbeitung der Eingabedaten die gewünschten Ergebnisse erzielt werden.

Der Name der neuronalen Netze leitet sich von im Gehirn vorzufindenden Neuronen ab, die für den Gedankenfluss verantwortlich sind. Das Erlernen von Parametern zur Steuerung von Ausgaben ist der Arbeitsweise von Neuronen nachempfunden. Der ursprüngliche Hintergedanke neuronaler Netze ist die technische Replikation der Arbeitsweise von Gehirnen.

\vspace*{-0.25cm}

\subsubsection{Convolutional Neural Networks (CNNs)}
\label{sec:cnns}

Die Art der verarbeiteten Daten in einem neuronalen Netz kann viele Formen annehmen. Insbesondere die Verarbeitung von Bilddaten ist ein großer Themenbereich neuronaler Netze und essenziell für diese Thesis. Bereits in \autoref{sec:was_filterung} wurde die Faltung auf Bilddaten eingeführt, die unter Verwendung von Kerneln geschieht. Auf dieser Arbeitsweise fußen die Convolutional-Schichten. Die Parameter dieser Schichten bestimmen die Ausprägung eines Kernels, welcher auf eingehende Bilddaten angewandt wird. Durch vielfache Hintereinanderreihung von Convolutional-Schichten werden inkrementell komplexere Strukturen in Bildern identifiziert und abstrakt festgehalten \cite{alexnet}. Neuronale Netze, die auf Convolutional-Schichten aufbauen, werden als \acp{cnn} bezeichnet \cite{cv_general}.

\vspace*{-0.25cm}

\subsubsection{Klassifizierung und Regression}
\label{sec:klassifizierung_regression}

Ebenso komplex wie Eingabedaten neuronaler Netze können ihre Ausgaben sein. Allgemein lassen sich Ausgaben von neuronalen Netzen in zwei Kategorien einteilen: Klassifikation und Regression \cite{nn_terminology,cv_general}.  % \footnote{Neben Klassifikation und Regression sind weitere Arten von Ausgaben möglich, beispielsweise Embeddings von Autoencodern. Für den Kontext dieser Arbeit sind diese Arten der Ausgaben jedoch nicht relevant, weshalb sich auf die gängigen Ausgaben klassischer neuronaler Netze beschränkt wird.} \cite{nn_terminology,cv_general}.
Bei der Klassifikation werden Datenpunkte in Form von Klassen vorhergesagt. Bei der Klassifizierung von Bildern sind Netzwerkausgaben der Klassen unterschiedlicher Objekte, Lebewesen oder Eigenschaften möglich (Beispielsweise die Beantwortung der Frage \quotes{\textit{Welches} Tier ist in diesem Bild zu sehen?}). Ebenso ist eine binäre Klassifikation hinsichtlich der Existenz bestimmter Sachverhalte üblich (Beispielsweise die Beantwortung der Frage \quotes{\textit{Existiert} eine Katze in diesem Bild?}). Die Ausgabe von neuronalen Netzen geschieht für diese Arten der Fragen typischerweise in Form von Vektoren, die diese Kategorien durch One-Hot-Encoding (oder 1-of-n-Encoding) darstellen \cite{one_hot_encoding}. Dabei ist jeder Kategorie ein Eintrag im Vektor zugeordnet; die Größe der Zahlenwerte geben die Sicherheit des Netzes für die jeweiligen Kategorien an.

Konträr zur Vorhersage diskreter Klassen ist die Vorhersage kontinuierlicher Werte als Ausgabe eines neuronalen Netzes möglich, bezeichnet als Regression. Beispiele für Ausgaben einer Regression beinhalten Funktionswerte oder Koordinaten. Ziel einer Regression ist es, konkrete Zahlenwerte vorherzusagen. Sofern eine Begrenzung der ausgegebenen Werte möglich ist, ist die Normalisierung von Daten in die Intervalle $[0, 1]$ oder $[-1, 1]$ üblich.

In dieser Thesis werden sowohl binäre als auch klassenbezogene Klassifikation sowie Regression verwendet. Die binäre Klassifikation wird zur Identifizierung von Dartpfeilen genutzt, klassenbezogene Klassifikation zur Identifizierung von Feldfarben unter Dartpfeilen und Regression wird genutzt, um die exakten Positionen der Dartpfeile auf der Dartscheibe darzustellen.

% -------------------------------------------------------------------------------------------------


\subsection{Training neuronaler Netze}
\label{sec:was_nn_training}

Das Training neuronaler Netze kann abhängig von der vorliegenden Art der Daten auf unterschiedliche Weisen verlaufen. In dieser Arbeit wird Supervised Learning verwendet, bei welchem Eingabedaten mit ihren zugehörigen Ausgaben gegeben sind. Das Netzwerk erlernt auf Grundlage dieser Daten Parameter, welche die Verarbeitungsschritte des Netzwerks steuern, um die Ausgaben aus den Eingabedaten abzuleiten \cite{nn_terminology}. Supervised Learning basiert auf der Korrektur von Fehlern getätigter Vorhersagen des neuronalen Netzes. Der Fehler von Vorhersagen wird durch eine Metrik gemessen, die hinsichtlich der Parameter des Netzwerks differenzierbar ist. Diese Metrik wird als Loss-Funktion\footnote{Alternativ wird diese Funktion auch als Cost- oder Error-Funktion bezeichnet. In dieser Arbeit wird die Terminologie der Loss-Funktion verwendet.} bezeichnet, der Fehler des Netzwerks als Loss. Durch die Differenzierbarkeit ist ihr Gradient bekannt und kann genutzt werden, um lokale Minima zu identifizieren. Je geringer der Fehler ist, desto korrekter sind die Vorhersagen des Systems. Das Erreichen eines Minimums der Loss-Funktion ist das Ziel des Trainings. Die Identifizierung der Parameterangleichungen zur Annäherung an ein Minimum der Loss-Funktion geschieht durch einen Prozess, der als Backpropagation bezeichnet wird. Während der Backpropagation werden basierend auf dem Loss Parameter der Netzwerkschichten angepasst, um die Vorhersage für die gegebenen Daten derart zu korrigieren, dass zukünftige Vorhersagen auf denselben Daten einen geringeren Loss mit sich ziehen \cite{cv_general}.

Implementierungen der Backpropagation und die Umsetzungen der Parameteranpassung geschehen durch Optimierungsalgorithmen. Die Arbeitsweisen dieser Algorithmen sind grundlegend ähnlich hinsichtlich der Eingaben des Problems, in den konkreten Arbeitsweisen unterscheiden sie sich jedoch. Die Auswahl eines passenden Optimierungsalgorithmus ist abhängig von der zugrundeliegenden Aufgabe und der Netzwerkarchitektur.

% -------------------------------------------------------------------------------------------------

\subsection{Terminologie}
\label{sec:nn_terminologie}

Der Themenbereich der neuronalen Netze umfasst eine Vielzahl von Konzepten und Begriffen. Dieses Unterkapitel gibt einen Überblick über die zentralen Begriffe, die in dieser Arbeit von Bedeutung sind \cite{nn_terminology}.

\paragraph{Trainingsdaten}

Trainingsdaten haben üblicherweise die größte Kardinalität aller für das Training und die Evaluation verwendeten Datensätze. Um ein effektives Training eines neuronalen Netzes zu gewährleisten, ist die Wahl der Trainingsdaten essenziell. Da der Trainingserfolg eines neuronalen Netzes abhängig von den Trainingsdaten ist und die durch die Parameter erlernten Strukturen in den Daten nicht bekannt sind, ist eine möglichst uniforme Abdeckung der zugrundeliegenden Daten wichtig. Jegliche Verzerrungen der Datenlage wird potenziell von dem neuronalen Netz erlernt und kann zu einer fehlerhaften Inferenz auf neuen Daten führen.

\paragraph{Validierungsdaten}

Validierungsdaten werden neben Trainingsdaten während des Trainings eines neuronalen Netzes genutzt, konträr zu Trainingsdaten haben diese jedoch keinen Einfluss auf den Trainingserfolg. In regelmäßigen Intervallen wird der aktuelle Stand der Netzwerkparameter auf den Validierungsdaten ausgewertet, um Einblicke in die Performance des Netzwerks auf Daten zu gewinnen, die nicht für das Training verwendet werden. Durch diese Vorhersagen können Rückschlüsse auf die Fähigkeit des neuronalen Netzes gezogen werden, das Gelernte auf neue Daten zu übertragen und die zugrundeliegenden Strukturen der Daten zu generalisieren. Die strikte Separierung von Trainings- und Validierungsdaten ist dabei obligatorisch, um eine verzerrte Ableitung der Fähigkeit zur Generalisierung zu vermeiden \cite{nn_terminology}.

Die Wahl der Validierungsdaten unterliegt den gleichen Voraussetzungen wie den Trainingsdaten. Darüber hinaus sollten Validierungsdaten jedoch auf eine Art und Weise gewählt werden, die nicht zu große Ähnlichkeiten zu den Trainingsdaten aufweist, da diese Nähe der Daten ebenfalls eine Verzerrung der Datenlage mit sich ziehen kann, sofern keine uniforme Verteilung der Trainingsdaten vorliegt.

\paragraph{Testdaten}

Nach dem Training eines neuronalen Netzes werden Testdaten genutzt, um die Performance des trainierten neuronalen Netzes zu evaluieren. Während Trainings- und Validierungsdaten Einfluss auf den Verlauf des Trainings nehmen, werden Testdaten genutzt, um einen unabhängigen Einblick in die Netzwerkperformance nach Beendigung des Trainings zu gewinnen \cite{nn_terminology}. Durch Testdaten wird die Inferenz des trainierten Netzes auf unbekannten Daten simuliert, wodurch eine objektive Abschätzung der Generalisierbarkeit ermöglicht wird.

Für die Wahl der Testdaten sind dieselben Voraussetzungen zu beachten, die für Trainings- und Validierungsdaten gelten, um einer Verzerrung der Datenlage vorzubeugen. Die Wahl von Trainings-, Validierungs- und Testdaten spielt für die Auswertung des neuronalen Netzes dieser Thesis eine wichtige Rolle.

\paragraph{\Acl{ood}-Training}

Dass die für das Training verwendeten Daten eine universelle Abdeckung über die gesamte Datenlage geben, ist häufig nicht möglich. Verzerrungen der Datenlage sind -- gewollt oder ungewollt -- in den meisten Fällen nicht zu umgehen. Weichen die Trainingsdaten jedoch signifikant von den Validierungs- und Testdaten ab, spricht man von \Acf{ood}-Training. Das neuronale Netz wird auf Daten trainiert, die einer anderen Verteilung entsprechen als der zu erwartenden Daten für die Inferenz des Netzes. Dies kann Auswirkungen auf die Inferenz des Systems auf Daten haben, die anders strukturiert sind als die für das Training verwendeten Daten.

\paragraph{Under- und Overfitting}

Die Validierungsdaten eines Trainings werden verwendet, um den Erfolg eines Trainings zu beurteilen. Während Optimierungsalgorithmen darauf ausgelegt sind, den Loss auf Trainingsdaten zu minimieren, ist es möglich, dass der Validierungs-Loss von diesem abweicht. Befindet sich der Wert des Validierungs-Loss konsequent und signifikant über dem Trainings-Loss, spricht man von Underfitting \cite{nn_terminology,cv_general}. In dieser Situation ist das neuronale Netz nicht in der Lage, das Gelernte auf unbekannte Daten anzuwenden, da es noch nicht ausreichend trainiert ist. Fallen Trainings- und Validierungs-Loss zeitweise gleichermaßen, gefolgt von einem Anstieg des Validierungs-Losses, wird dies als Overfitting bezeichnet \cite{cv_general}. In dieser Situation werden Vorhersagen auf den Trainingsdaten besser, jedoch verliert das neuronale Netz die Fähigkeit der Generalisierbarkeit gelernter Strukturen auf neue Daten. Es werden nicht mehr die relevanten Aussagen hinter den Daten erlernt, sondern die konkreten Ausprägungen in den Trainingsdaten.

% -------------------------------------------------------------------------------------------------

\subsection{Augmentierung von Trainingsdaten}
\label{sec:was_augmentierung}

Für ein robustes Training eines neuronalen Netzes und für die Vermeidung von Overfitting ist eine große und möglichst umfangreiche Datenlage notwendig. Zur künstlichen Vervielfältigung der für das Training verwendeten Daten kann eine Technik genutzt werden, die als Augmentierung bezeichnet wird \cite{cv_general}. Unter Augmentierung wird eine Manipulation der Trainingsdaten bezeichnet, bei der die Datenmenge vervielfältigt wird, ohne die Integrität der Daten zu beeinträchtigen.

Beispiele für Augmentierung von Bilddaten sind die Anwendung affiner Transformationen oder das Hinzufügen von Rauschen. Sofern die Magnitude dieser Manipulationen nicht derart groß ist, dass die relevanten Aussagen der Bilddaten unterdrückt werden, ist die Erschaffung neuer Grunddaten aus Bildern mit bekannten Annotationen möglich. Ein robust trainiertes neuronales Netz ist in der Lage, die relevanten Informationen aus den Bildern zu extrahieren und eine Invarianz gegenüber der in der Augmentierung angewandten Operationen zu entwickeln. Bei dem Overfitting eines Netzwerks ist diese Invarianz gegenüber der Augmentierungsoperationen nicht gegeben.

Durch diese Technik wird eine größere Bandbreite möglicher Parameter und resultierend mögliche Eingaben in das System abgedeckt \cite{augmentierung, augmentation_max_likelihood_est}. Die Verwendung von Datenaugmentierung resultiert tendenziell in einer Verbesserung der Netzwerkperformance \cite{augmentierung_auswirking}.

% -------------------------------------------------------------------------------------------------

\newpage
\subsection{Die YOLOv8-Architektur}
\label{sec:was_yolov8}

YOLOv8 ist eine Netzwerkarchitektur, die 2023 durch Ultralytics außerhalb des Rahmens einer gesonderten wissenschaftlichen Aufarbeitung veröffentlicht wurde\footnote{Siehe: \url{https://github.com/ultralytics/ultralytics/issues/2572}}. Mit YOLOv8 ist eine Netzwerkarchitektur vorgestellt, die Objekterkennung durch Anchor-Free-Erkennungsmechanismen in Echtzeit und in hoher Genauigkeit ermöglicht \cite{yolov1_to_yolov8}. Sie wird als Verbesserung der YOLOv5-Architektur vorgestellt und ist für den Einsatz in Edge-Devices optimiert \cite{yolov8_comparison}.

Der Aufbau von YOLOv8 folgt einem Design, welches in Backbone, Neck und Head unterteilt ist. Als Backbone wird eine erweiterte Version des CSPDarknet-Backbones verwendet während der Neck auf einem PANet basiert. Mit dem Aufbau von YOLOv8 wird das Paradigma des \acf{fcnn} verfolgt, welches die Verarbeitung von Bilddaten beliebiger Eingabegrößen ermöglicht.

Die Ausgaben des Netzwerks verfolgen einen Multi-Scale-Ansatz, durch welchen die Erkennung von Objekten unterschiedlicher Größen durch Rasterung der Eingabebilder in unterschiedliche Größen geschieht. Das Bild wird in Zellen unterschiedlicher Skalierungen unterteilt und es werden Vorhersagen für jede Zelle ausgegeben hinsichtlich Mittelpunkt eines Objekts, seiner Klasse und der Ausdehnung des Objekts über das Bild durch die Angabe einer Bounding Box. Die Ausgaben des Netzwerks werden durch etablierte \ac{nms} nachverarbeitet.

Der Einsatzbereich von YOLOv8 spezifiziert sich auf Echtzeitanwendungen, in welchen robuste und akkurate Objekterkennung gewünscht ist und in welchen die zur Verfügung stehenden Ressourcen für das Deployment dieses Netzwerks begrenzt sind. Beispiele solcher Anwendungen sind der Einsatz in autonomen Fahrzeugen oder zur Objekterkennung in Drohnen \cite{yolov8_car,yolov8_drone}.

% -------------------------------------------------------------------------------------------------
