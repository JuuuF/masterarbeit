% !TeX root = ../main.tex

\section{Implementierung}
\label{sec:ki:implementierung}

Nach der Darstellung der Methodik wird in diesem Abschnitt auf Details zur Implementierung eingegangen. Dieser Abschnitt ist unterteilt in zwei Unterabschnitte: Im ersten Unterabschnitt werden konkrete Details zur Implementierung der YOLOv8*-Architektur thematisiert, der zweite Unterabschnitt befasst sich mit den spezifischen Aspekten des Trainings.

% -------------------------------------------------------------------------------------------------

\subsection{Implementierung der YOLOv8*-Architektur}
\label{sec:yolov8_implementierung}

Vortrainierte neuronale Netze der YOLO-Familie werden als Modelle veröffentlicht, die mit dem Framework PyTorch erstellt sind. Für diese Thesis wird TensorFlow als Framework für die Erstellung und das Training neuronaler Netze verwendet, welches nicht reibungslos mit PyTorch vereinbar ist. Die Frameworks arbeiten auf unterschiedliche Arten, wodurch eine Übersetzung der verwendeten Schichten und der vortrainierten Gewichte zwischen diesen lediglich bedingt möglich ist. Durch die Abwandlung von YOLOv8* zu YOLOv8 ist eine eigene Implementierung notwendig und eine Einbettung vortrainierter Gewichte in diese Architektur ist nicht trivial möglich. Auf Grundlage der offiziellen Dokumentation sowie des Quelltexts und Konfigurationsdateien wurde die YOLOv8-Architektur in TensorFlow übersetzt und implementiert. Atomare Bestandteile wie Convolution, Batch-Normalisierung, Pooling-Operationen und Aktivierungsschichten können analog von PyTorch zu TensorFlow übertragen werden. Schichten, die nicht in TensorFlow verfügbar sind -- beispielsweise die Split-Operation -- wurden durch Einbindung eigener Schichten implementiert.

Nachdem alle grundlegenden Bestandteile verfügbar waren, wurden aus diesen kombinierte Netzwerkbestandteile zusammengesetzt, dargestellt in \autoref{img:yolov8_parts}. Die Dimensionierungen der jeweiligen Schichten sowie die verwendeten Parameter konnten aus der Dokumentation der Architektur entnommen und analog übertragen werden. Nachdem alle Bestandteile der Architektur implementiert waren, wurde eine generische Implementierung der Architektur vorgenommen, in der die bereitgestellten Parameter für unterschiedliche Größenvariationen von YOLOv8 mit einbezogen wurden. Durch den Vergleich der Anzahl der Netzwerkparameter unterschiedlicher Größenkonfigurationen sowie der Analyse des Netzwerkaufbaus durch Verbindungen der Schichten konnte sichergestellt werden, dass die TensorFlow-Implementierung von YOLOv8 der vorgestellten Architektur entsprach.

Nachdem die grundlegende Architektur in TensorFlow verfügbar war, wurde sie durch eigene Adaptionen erweitert. Eine grundlegende Änderung ist das Hinzufügen von Dropout-Schichten, welche nicht in der Dokumentation erwähnt sind, jedoch die Stabilität des Trainings erhöhen und die Wahrscheinlichkeit des Overfittings senken. Eine weitere Adaption ist das Hinzufügen von Transition-Blocks, welche eine residuale Dense-Schicht umfassen. In einem Transition-Block wird der Eingabetensor durch einen Tensor moduliert, der den globalen Kontext des Eingabetensors durch eine Dense-Schicht einfängt.

% -------------------------------------------------------------------------------------------------

\subsection{Training von YOLOv8* zur Identifizierung von Dartpfeilen}
\label{sec:training}

In diesem Unterabschnitt wird das Training des neuronalen Netzes thematisiert. Es wird begonnen mit der verwendeten Infrastruktur und Rahmenbedingungen des Trainings. Danach folgt eine detaillierte Betrachtung der verwendeten Augmentierungsparameter. Zuletzt wird der Verlauf des Trainings erläutert.

\subsubsection{Infrastruktur und Rahmenbedingungen}

Das Training von YOLOv8* wurde auf einer NVIDIA GeForce RTX 4090 aus einem TensorFlow-Docker-Container ausgeführt. Es wurde eine Batch-Size von $32$ verwendet mit dem AdamW-Optimizer und einer dynamischen Learning Rate, wie in \autoref{sec:dynamisches_training} erläutert. Für das Training wurden $24.960$ Trainings- und $672$ Validierungsdaten verwendet. Die Trainingsdaten setzen sich zusammen aus $24.576$ generierten Daten ($20.480$ Daten auf Grundlage regulärer Heatmaps und $4.096$ Daten auf Grundlage von Multiplier-Heatmaps), $256$ Daten des DeepDarts-$d_1$-Trainingssatzes und $128$ manuell aufgenommenen Daten. Die Validierungsdaten sind zusammengesetzt aus $256$ synthetischen Daten, $256$ Daten der DeepDarts-$d_2$-Trainingsdaten sowie $160$ manuell aufgenommen Daten. Die Trainings- und Validierungsdaten unterliegen einer strikten thematischen Trennung, sodass die realen Datensätze in unterschiedlichen Umgebungen aufgenommen wurden. Auf diese Weise wird Darstellung der Generalisierbarkeit durch den Validation-Loss nicht durch eine vorteilhafte Datenlage verzerrt.

\subsubsection{Augmentierungsparameter}

Die Daten werden vektorisiert als TensorFlow Datasets eingelesen, wodurch ein effizientes und parallelisiertes Laden der Daten ermöglicht wird. Ein Schritt des Einlesens der Daten ist die Augmentierung, welche dynamisch auf jedes Bild der Trainingsdaten mit zufälligen Parametern angewendet wird. Die Farbkanäle für rot, grün und blau werden unabhängig voneinander mit einem zufälligen, uniform gewählten Gewicht $A_{\text{cha}, \{r,g,b\}} \in [0,\!5\,..\,1]$ moduliert. Die Helligkeit des Bildes wird durch den Parameter $A_b$ bestimmt, der die mittlere Helligkeit des Bildes zufällig uniform verteilt in dem Intervall $[\text{min}(0.1, b_\text{img}-b_\text{adj}), b_\text{img} + b_\text{adj}]$ setzt, wobei $b_\text{img}$ die mittlere Helligkeit des Bildes ist und $b_\text{adj} = 0,\!03$ die maximale Änderung der Helligkeit vorgibt. Der Kontrast des Bildes wird durch den zufällig uniform gewählten Parameter $A_\text{cont} \in [0,\!7\,..\,1,\!3]$ bestimmt und die Farbsättigung wird um einen ebenfalls uniform verteilten Faktor $A_\text{sat} \in [0,\!8\,..\, 1,\!2]$ moduliert. Das Hinzufügen von normalverteiltem Rauschen auf jeden Pixel jedes Farbkanals findet mit einer Normalverteilung $\sigma_\text{noise} = 0,\!15$ statt. Hinsichtlich der transformativen Augmentierungsparameter wird das Bild horizontal und vertikal mit einer Wahrscheinlichkeit von je $50\,\%$ gespiegelt. Eine Rotation um den Mittelpunkt des Bildes erfolgt zufällig um einen Winkel aus der Menge $A_\text{rot} \in \{i \in \mathbb{N} ~\vert~ i = n \cdot 18\degree, n \in \mathbb{N}_{<20}\}$. Zuletzt erfolgt eine normalverteilte Translation des Bildes $A_\text{trans} \in \mathbb{R}^2$ mit einer Standardabweichung von $\sigma_\text{trans} = 5\,\text{px}$.
\nomenclature{$A_{\text{cha}, \{r,g,b\}}$}{Augmentierungsgewichte für rot-, grün- unb blau-Kanäle.}
\nomenclature{$A_\text{cont}$}{Augmentierungsgewicht der Kontraständerung.}
\nomenclature{$A_\text{sat}$}{Augmentierungsgewicht der Sättigungsänderung.}
\nomenclature{$A_\text{rot}$}{Augmentierungsgewicht der Rotation.}
\nomenclature{$b_\text{img}$}{Mittlere Helligkeit eines Bildes.}
\nomenclature{$b_\text{adj}$}{Maximale Helligkeitsänderung der Augmentierung.}
\nomenclature{$\sigma_\text{noise}$}{Standardabweichung der Rausch-Augmentierung.}
\nomenclature{$\sigma_\text{trans}$}{Standardabweichung der Translations-Augmentierung.}
