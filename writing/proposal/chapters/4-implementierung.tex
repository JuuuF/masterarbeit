\section{Implementierung}
\label{sec:implementierung}
Die Implementierung gliedert sich in mehrere Schritte, die in den folgenden Unterkapiteln jeweils detailliert dargestellt werden, um den Entwicklungsprozess präzise zu beschreiben.

\subsection{Datenerstellung}
\label{sec:implementierung:datenerstellung}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{imgs/blender.png}
    \caption{Screenshot des Blender-Projekts zur Generierung synthetischer Aufnahmen von Dartscheiben.}
    \label{img:blender}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{imgs/render.png}
    \caption{Exemplarisches Beispiel eines synthetisch erstellten Datensatzes.}
    \label{img:render}
\end{figure}

Zur Generierung synthetischer Daten wird die Open-Source-Software Blender verwendet \cite{blender}, die sich durch ihre umfangreichen Möglichkeiten zur Erstellung fotorealistischer 3D-Szenen auszeichnet. Mit Blender lassen sich detaillierte Szenen modellieren, die Dartscheiben und Pfeile in verschiedenen Umgebungen darstellen. Ein Beispiel für eine erstellte Szene ist in \autoref{img:blender} zu sehen, das Ergebnis eines Renderings in \autoref{img:render}. Durch Parameter-gestützte Erstellung von Objekten und Szenen ist eine modulare und vielfältige Variation der Renderings möglich, durch welche eine große Spanne verschiedener Szenarien simuliert werden kann.

Der Zugriff auf die Blender-Szene wird durch die Python-Bibliothek \textit{bpy} ermöglicht \cite{bpy}. Diese erlaubt eine vollständige Automatisierung der Datenproduktion ohne grafische Benutzeroberfläche. Mithilfe von Skripten können Parameter wie Position, Beleuchtung und Texturen von Objekten sowie Kameraparameter manipuliert werden. Besonders vorteilhaft ist, dass dieser Prozess keine lokale Blender-Installation erfordert und dadurch auf GPU-Servern durchgeführt werden kann, wodurch eine flexible Skalierung und Verarbeitung großer Datenmengen möglich ist.

Das Variieren von Parametern, wie der Lichtverhältnisse, der Oberflächenbeschaffenheit oder Kameraposition, erlaubt die realistische Nachbildung unterschiedlicher Aufnahmebedingungen. Eine hohe Diversität wird durch die Simulation verschiedener Lichtquellen, Raumkonfigurationen und Texturen erreicht. Parameter können dabei sowohl zufällig aus einer vordefinierten Spanne an Werten, als auch anhand von Heatmaps und Wahrscheinlichkeitsverteilungen gesetzt werden. Ein weiterer wichtiger Aspekt der Datenerstellung ist die korrekte Annotation der synthetischen Bilder. Da die Daten aus einer virtuellen Umgebung generiert werden, ist sichergestellt, dass jede Bild-Pixel-Annotation präzise und korrekt ist. Dies ermöglicht eine effiziente und fehlerfreie Vorbereitung der Daten für das Training.

\subsection{Erkennung der Dartscheibe}
\label{sec:implementierung:dartscheibe}

Die Dartscheibe stellt durch ihren strukturierten und kontrastreichen Aufbau eine gute Grundlage für Bildverarbeitungsalgorithmen dar. Der universelle Aufbau aus radial angeordneten Segmenten und Ringen, die klar unterscheidbare Farben und Formen aufweisen, sind eine gute Voraussetzung für Verfahren zur Dartscheiben-Detektion \cite{dra-darts-rules, wdf-darts-rules}.

Für die Erkennung wird auf klassische Methoden der Computer Vision zurückgegriffen, wie etwa Farbraumtransformationen, Kantendetektion und die Hough-Transformation zur Kreis- und Ellipsenerkennung. Durch diese Verfahren kann die Geometrie der Dartscheibe extrahiert und anschließend eine Entzerrung der perspektivischen Aufnahme vorgenommen werden. Diese klassischen Verfahren können durch den Einsatz neuronaler Netze, insbesondere CNNs, ergänzt werden, um eine robuste Erkennung zu gewährleisten, z.B. wenn Teile der Dartscheibe verdeckt sind. Für die Implementierung von Computer-Vision-Techniken wird die Bibliothek \textit{OpenCV} verwendet \cite{opencv}.

\subsection{Training neuronaler Netze zur Erkennung von Dartpfeilen}
\label{sec:implementierung:ki}

Für die Lokalisierung der Dartpfeile wird ein neuronales Netz entwickelt, das mit synthetischen und realen Daten trainiert wird. Die Implementierung erfolgt mit dem \textit{TensorFlow}-Framework, das umfassende Möglichkeiten zur Datenverarbeitung bietet und speziell durch seine Optimierung für das Training auf GPUs eine effiziente Handhabung großer Datenmengen erlaubt \cite{tensorflow}.

Die Trainingsdaten werden in drei Hauptkategorien unterteilt: Trainings-, Validierungs- und Testdaten.
Ein Großteil der Trainingsdaten wird aus synthetischen Daten bestehen, während zur Validierung hauptsächlich reale Daten verwendet werden. Für die Testphase wird auf Datensätze zurückgegriffen, die im Referenzpaper verwendet wurden. Diese strukturierte Aufteilung der Daten gewährleistet einen aussagekräftigen Vergleich zwischen dem entwickelten System und dem Referenzmodell von McNally et al.

Ein zentraler Punkt zur Verbesserung der Modellleistung ist die Datenaugmentierung. Diese umfasst sowohl pixelbasierte Transformationen der Eingabedaten, wie das Hinzufügen von Rauschen oder die Anpassung von Helligkeit und Kontrast, als auch geometrische Transformationen der Eingabe- und Ausgabedaten, wie Spiegeln, Rotieren oder Skalieren der Bilder. Wichtig hierbei ist, dass Eingangs- und Ausgangsdaten stets synchron transformiert werden, um eine Verfälschung der Daten zu verhindern. Durch die Einführung solcher Augmentierungen wird das Modell auf eine Vielzahl von realistischen Szenarien vorbereitet, was das Risiko von Overfitting\footnote{Als Overfitting wird das Auswendiglernen von Daten bezeichnet. Es zeichnet sich durch gute Performance in Trainingsdaten aus, während die Fähigkeit zur Generalisierung auf unbekannte Daten sinkt.} erheblich reduziert.

Diese Ansätze gewährleisten, dass das entwickelte Modell in der Lage ist, robuste und präzise Vorhersagen zur Position der Dartpfeile zu treffen. Darüber hinaus legen sie den Grundstein für eine systematische Verbesserung gegenüber dem ursprünglichen Ansatz.

\iffalse
\begin{enumerate}
    \item Datenerstellung
    \begin{itemize}
        \item Blender, siehe \autoref{img:blender}, \autoref{img:render}
        \item Modell von Dartscheibe, Raum, Lichtern und mehreren Arten von Pfeilen

        \item Zugriff auf Szene mittels "bpy"
        \begin{itemize}
            \item Python-Framework zur Interaktion mit Blender
            \item \glqq API\grqq\, für Blender-Szenen
            \item Zugriff auf alle Objekte und Parameter per Python-Skript
        \end{itemize}

        \item Modelle durch Parameter einstellbar
        \begin{itemize}
            \raggedright
            \item Farbe und Beschaffenheit von Texturen
            \item Lichtverhältnisse (Helligkeit, Farbe etc.) (Blitzlicht, HDRI, Deckenlampen, ...)
            \item Abnutzung des Boards
            \item Aussehen der Dartpfeile
            \item Umgebung um das Board / Raumausstattung (Dartboard-Schrank, Bilder, weitere Dartscheiben im Bild, ...)
        \end{itemize}

        \item Parameter zufällig oder gewichtet setzen
        \begin{itemize}
            \item Kameraposition z.B. zufällig in definiertem Raum
            \item Pfeilpositionen z.B. anhand von Heatmaps gewichtet positionieren
        \end{itemize}

        \item Korrektheit der Daten gesichert
        \begin{itemize}
            \item 3D-Positionen von Dartpfeilen und Scheibe bekannt $\rightarrow$ Feldwerte können analytisch bestimmt werden
            \item Statistik nicht nur über Feldwert, sondern exakte Positionen möglich
        \end{itemize}

        \item Rendering auf GPU-Server, da bpy keine GUI benötigt
    \end{itemize}

    \item Erkennung der Dartscheibe
    \begin{itemize}
        \raggedright
        \item Große Flächen, hoher Kontrast
        \item bestehend aus Primitiven: Kreise, Linien, Dreiecke
        \item Verzerrung: Kreise $\rightarrow$ Ellipsen
        \item insgesamt gute Voraussetzungen, um mittels herkömmlicher CV-Methoden erkannt zu werden $\rightarrow$ OpenCV
        \item Ggf. simples CNN zur Identifizierung von Dartscheiben: Daten ebenfalls durch 3D-Rendering erstellbar
        \item Fisheye-Effekte könnten problematisch sein
    \end{itemize}

    \item KI-Training:
    \begin{itemize}
        \item Training mit TensorFlow auf GPU-Server
        \item Datenaufteilung:
        \begin{itemize}
            \item Trainingsdaten: Synthetische Daten (Masse, korrekt annotiert)
            \item Validierungsdaten: Reale Daten, eigene
            \item Testdaten: Baseline-Daten
            \item Argumentierung: Wir sind auf deren Daten besser als sie selbst, ohne auf den Daten trainiert zu haben
        \end{itemize}

        \item Datenaugmentierung
        \begin{itemize}
            \item Pixel-Augmentierung: Noise, Helligkeit, Kontrast, ...
            \item Transformations-Augmentierung: Rotation, Skalierung, Flips, ...
        \end{itemize}

        \item Ggf. Hyperparameter-Tuning durch Random Grid / Bayesian Search
        \item Ggf. k-Fold Cross-Validation
    \end{itemize}

\end{enumerate}
\fi
