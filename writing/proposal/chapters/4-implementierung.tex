\section{Implementierung}
\label{sec:implementierung}
Die Implementierung gliedert sich in mehrere Schritte, die in den folgenden Unterkapiteln jeweils detailliert dargestellt werden, um den Entwicklungsprozess präzise zu beschreiben.

\subsection{Datenerstellung}
\label{sec:implementierung:datenerstellung}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{imgs/blender.png}
    \caption{Blender-Projekt}
    \label{img:blender}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{imgs/render.png}
    \caption{Render}
    \label{img:render}
\end{figure}

Zur Generierung synthetischer Daten wird die Open-Source-Software Blender verwendet, die sich durch ihre umfangreichen Möglichkeiten zur Erstellung fotorealistischer 3D-Szenen auszeichnet. Mit Blender lassen sich dataillierte Szenen modellieren, die Dartscheiben und Pfeile in verschiedenen Umgebungen darstellen. Ein Beispiel für eine erstellte Szene ist in \autoref{img:blender} zu sehen, das Ergebnis eines Renderings in \autoref{img:render}. Durch parametrisierbare Erstellung von Objekten und Szenen ist eine modulare und vielfältige Variation der Renderings möglich, durch welche eine große Spanne verschiedener Szenarien simuliert werden kann.

Der Zugriff auf die Blender-Szene wird durch die Python-Bibliothek \textit{bpy} ermöglicht. Diese erlaubt eine vollständige Automatisierung der Datenproduktion ohne grafische Benutzeroberfläche. Mithilfe von Skripten können Parameter wie Position, Beleuchtung und Texturen von Objekten sowie Kameraparameter manipuliert werden. besonders vorteilhaft ist, dass dieser Prozess ohne lokale Blender-Installation erfordert und dadurch auf GPU-Servern durchgeführt werden kann, wodurch eine flexible Skalierung und Verarbeitung großer Datenmengen möglich ist.

Das Variieren von Parametern, wie der Lichtverhältnisse, der Oberflächenbeschaffenheit oder Kameraposition, erlaubt die realistische Nachbildung unterschiedlicher Aufnahmebedingungen. Eine hohe Diversität wird durch die Simulation verschiedener Lichtquellen, Raumkonfigurationen und Texturen erreicht. Parameter können dabei sowohl zufällig aus einer vordefinierten Spanne an Werten, wie auch anhand von Heatmaps und Wahrscheinlichkeitsverteilungen gesetzt werden. Ein weiterer wichtiger Aspekt der Datenerstellung ist die korrekte Annotation der synthetischen Bilder. Da die Daten aus einer virtuellen Umgebung generiert werden, ist sichergestellt, dass jede Bild-Pixel-Annotation präzise ist. Dies ermöglicht eine effiziente und fehlerfreie Vorbereitung der Daten für das Training.

\subsection{Erkennung der Dartscheibe}
\label{sec:implementierung:dartscheibe}

Die Dartscheibe stellt durch ihren strukturierten und kontrastreichen Aufbau eine ideale Grundlage für Bildverarbeitungsalgorithmen dar. Der universelle Aufbau aus radial angeordneten Segmenten und Ringen, die klar unterscheidbare Farben nd Formen aufweisen, erlaubt es, robuste Verfahren zur Dartscheiben-Detektion einzusetzen.

Für die Erkennung wird auf klassische Methoden der Computer Vision zurückgegriffen, wie etwa Kantendetektion und die Hough-Transformation zur Kreis- und Ellipsenerkennung. Durch diese Verfahren wird die Geometrie der Dartscheibe extrahiert und anschließend eine Entzerrung der perspektivischen Aufnahme vorgenommen. Diese klassischen Verfahren können durch den Einsatz neuronaler Netze, insbesondere CNNs, ergänzt werden, um eine robuste Erkennung auch unter schwierigen Bedingungen zu gewährleisten, beispielsweise wenn Teile der Dartscheibe verdeckt sind.

\subsection{Training neuronaler Netze zur Erkennung von Dartpfeilen}
\label{sec:implementierung:ki}

Für die Lokalisierung der Dartpfeile wird ein neuronales Netz entwickelt, das mit synthetischen und realen Daten trainiert wird. Die Implementierung erfolgt mit dem TensorFlow-Framework, das umfassende Möglichkeiten zur Datenverarbeitung bietet und insbesondere durch seine Optimierung für das Training auf GPUs eine effiziente Handhabung großer Datenmengen erlaubt.

Die Trainingsdaten werden in drei Hauptkategorien unterteilt: Trainings-, Validierungs- und Testdaten.
Ein Großteil der Trainingsdaten wird aus synthetischen Daten bestsehen, während zur Validierung hauptsächlich reale Daten verwendet werden. Für die abschließende Testphase wird auf Datensätze zurückgegriffen, die im Referenzpaper verwendet wurden. Diese strukturierte Aufteilung der Daten gewährleistet einen aussagekrästigen Vergleich zwischen dem entwickelten System und dem Referenzmodell von McNally et al.

Ein zentraler Punkt zur Verbesserung der Modellleistung ist die Datenaugmentierung. Diese umfasst sowohl pixelbasierte Transformationen der Eingabedaten, wie das Hinzufügen von Rauschen oder die Anpassung von Helligkeit und Kontrast, als auch geometrische Transformationen der Eingabe- und Ausgabedaten, wie Spiegeln, Rotieren oder Skalieren der Bilder. Wichtig hierbei ist, dass Eingangs- und Ausgangsdaten stets synchron transformiert werden, um eine Verfälschung der Daten zu verhindern. Durch die Einführung solcher Augmentierungen wird das Modell auf eine Vielzahl von realistischen Szenarien vorbereitet, was das Risiko von Overfitting, dem Auswendiglernen von Daten, erheblich reduziert.

Diese Ansätze gewährleisten, dass das entwickelte Modell in der Lage ist, robuste und präzise Vorhersagen zur Position der Dartpfeile zu treffen, und legen den Grundstein für eine systematische Verbesserung gegenüber dem ursprünglichen Ansatz.

\iffalse
\begin{enumerate}
    \item Datenerstellung
    \begin{itemize}
        \item Blender, siehe \autoref{img:blender}, \autoref{img:render}
        \item Modell von Dartscheibe, Raum, Lichtern und mehreren Arten von Pfeilen

        \item Zugriff auf Szene mittels "bpy"
        \begin{itemize}
            \item Python-Framework zur Interaktion mit Blender
            \item \glqq API\grqq\, für Blender-Szenen
            \item Zugriff auf alle Objekte und Parameter per Python-Skript
        \end{itemize}

        \item Modelle durch Parameter einstellbar
        \begin{itemize}
            \raggedright
            \item Farbe und Beschaffenheit von Texturen
            \item Lichtverhältnisse (Helligkeit, Farbe etc.) (Blitzlicht, HDRI, Deckenlampen, ...)
            \item Abnutzung des Boards
            \item Aussehen der Dartpfeile
            \item Umgebung um das Board / Raumausstattung (Dartboard-Schrank, Bilder, weitere Dartscheiben im Bild, ...)
        \end{itemize}

        \item Parameter zufällig oder gewichtet setzen
        \begin{itemize}
            \item Kameraposition z.B. zufällig in definiertem Raum
            \item Pfeilpositionen z.B. anhand von Heatmaps gewichtet positionieren
        \end{itemize}

        \item Korrektheit der Daten gesichert
        \begin{itemize}
            \item 3D-Positionen von Dartpfeilen und Scheibe bekannt $\rightarrow$ Feldwerte können analytisch bestimmt werden
            \item Statistik nicht nur über Feldwert, sondern exakte Positionen möglich
        \end{itemize}

        \item Rendering auf GPU-Server, da bpy keine GUI benötigt
    \end{itemize}

    \item Erkennung der Dartscheibe
    \begin{itemize}
        \raggedright
        \item Große Flächen, hoher Kontrast
        \item bestehend aus Primitiven: Kreise, Linien, Dreiecke
        \item Verzerrung: Kreise $\rightarrow$ Ellipsen
        \item insgesamt gute Voraussetzungen, um mittels herkömmlicher CV-Methoden erkannt zu werden $\rightarrow$ OpenCV
        \item Ggf. simples CNN zur Identifizierung von Dartscheiben: Daten ebenfalls durch 3D-Rendering erstellbar
        \item Fisheye-Effekte könnten problematisch sein
    \end{itemize}

    \item KI-Training:
    \begin{itemize}
        \item Training mit TensorFlow auf GPU-Server
        \item Datenaufteilung:
        \begin{itemize}
            \item Trainingsdaten: Synthetische Daten (Masse, korrekt annotiert)
            \item Validierungsdaten: Reale Daten, eigene
            \item Testdaten: Baseline-Daten
            \item Argumentierung: Wir sind auf deren Daten besser als sie selbst, ohne auf den Daten trainiert zu haben
        \end{itemize}

        \item Datenaugmentierung
        \begin{itemize}
            \item Pixel-Augmentierung: Noise, Helligkeit, Kontrast, ...
            \item Transformations-Augmentierung: Rotation, Skalierung, Flips, ...
        \end{itemize}

        \item Ggf. Hyperparameter-Tuning durch Random Grid / Bayesian Search
        \item Ggf. k-Fold Cross-Validation
    \end{itemize}

\end{enumerate}
\fi
